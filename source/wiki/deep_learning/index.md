---
layout: wiki
wiki: DeepLearning
title: 认识深度学习
mathjax: true
references: 
  - title: MIT 深度学习 （github）
    url: https://github.com/janishar/mit-deep-learning-book-pdf
---

<!-- more -->

## 什么是深度学习 ?

深度学习的历史可以追溯到 20 世纪 40 年代。深度学习看似是一个全新的领域，只不过因为在目前流行的前几 年它是相对冷门的，同时也因为它被赋予了许多不同的名称（其中大部分已经不再使用），最近才成为众所周知的 “深度学习’’。这个领域已经更换了很多名称，它反映了不同的研究人员和不同观点的影响。一般来说，目前为止深度学习已经经历了三次发展浪潮：20 世纪 40 年代到 60 年代深度学习的雏形出现在 *控制论*（cybernetics）中，20 世纪 80 年代 到 90 年代深度学习表现为 *联结主义*（connectionism），直到 2006 年，才真正以**深度学习**之名复兴。

现代术语 “深度学习’’ 超越了目前机器学习模型的神经科学观点。它诉诸于学 习多层次组合这一更普遍的原理，这一原理也可以应用于那些并非受神经科学启发的机器学习框架。
深度学习最大的用处是他可以学习非线性函数。最经典的一个机器学习的例子是一个简单的单层神经网络就可以学习到XOR函数。现在，神经科学被视为深度学习研究的一个重要灵感来源，但它已不再是该领域的主要指导。

如今神经科学在深度学习研究中的作用被削弱，主要原因是我们根本没有足够 的关于大脑的信息来作为指导去使用它。要获得对被大脑实际使用算法的深刻理解，我们需要有能力同时监测（至少是）数千相连神经元的活动。我们不能够做到这一点，所以我们甚至连大脑最简单、最深入研究的部分都还远远没有理解。深度学习的另一个最大的成就是其在 强化学习（reinforcement learning）领域的扩展。在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来学习执行任务。DeepMind 表明，基于深度学习的强化学习系统能够学会玩 Atari 视频游戏，并在多种任务中可与人类匹敌 (Mnih et al., 2015)。深度学习也显著改善了机器人强化学习的性能 (Finn et al., 2015)。

总之，深度学习是机器学习的一种方法。在过去几十年的发展中，它大量借鉴了我们关于人脑、统计学和应用数学的知识。近年来，得益于更强大的计算机、更大的数据集和能够训练更深网络的技术，深度学习的普及性和实用性都有了极大的发展。未来几年充满了进一步提高深度学习并将它带到新领域的挑战和机遇。

## 深度学习经典实例：神经网络学习 XOR 算法

XOR 函数（"异或" 逻辑）是两个二进制值 `x1` 和 `x2` 的运算。当这些二进制值 中恰好有一个为 1 时，XOR 函数返回值为 1。其余情况下返回值为 0。XOR 函数提供了我们想要学习的目标函数 `y = f^* (x)`。我们的模型给出了一个函数 `y = f(x; θ)` 并且我们的学习算法会不断调整参数 `θ` 来使得 `f` 尽可能接近 `f^∗`。

在这个简单的例子中， 统计泛化并不重要。首先我们可以把这个问题当作是回归问题，并使用均方误差损失函数。我们选择这 个损失函数是为了尽可能简化本例中用到的数学。在应用领域，对于二进制数据建模时，MSE通常并不是一个合适的损失函数。

假设这是一个线性模型，损失函数我们用MSE，模型的目标函数是`f(x; w,b) = xw + b`. 我们可以使用正规方程来解w和b来试损失函数最小。求解以后得到w=0， b=1/2. 线性模型仅仅是在任意一点都输出 0.5。显然这不是我们想要的结果。

如果我们在此基础上加入一个隐藏层，这个隐藏层的激活函数我们用RELU（rectified linear unit）。这个模型就变成了 `f(x; w1, w2, b1, b2) = w1 * Max{0, w2 * x + b2} + b1`. 最后得到一个函数， 函数图像如下：

{% image https://cdn.jsdelivr.net/gh/zhenxiang-shawn/zhenxiang-shawn.github.io@main/source/_imgs/xor_nn_out.png XOR NN函数  %}

神经网络对这XOR函数的每个样本都给出了正确的结果。在这个例子中，我们简单地指定了解决方案，然后说明它得到的误差为零。在实际情况中，可能会有数十亿的模型参数以及数十亿的训练样本，所以不能像我们这里做的那样进行简单地猜解。与之相对的，基于梯度的优化算法可以找到一些参数使得产生的误差非常小。我们这里给出的 XOR 问题的解处在损失函数的全局最小点，所以梯度下降算法可以收敛到这一点。梯度下降算法还可以找到 XOR 问题一些其他的等价解。梯度下降算法的收敛点取决于参数的初始值。在实践中，梯度下降通常不会找到像我们这里给出的那种干净的、容易理解的、整数值的解。

## 强化学习
