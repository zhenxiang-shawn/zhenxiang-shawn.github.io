[{"title":"VSCode 连接 Docker 镜像失败","path":"/2024/01/30/2024-01-29-vscode-connect-docker-failed/","content":"买发时,有时需要直接在 docker 中开发或者 debug. 使用 VSCode 会方便很多. 但是有时候安装了 Remote Development 还是会报错: Failed to connect. Is docker running? 原因是docker使用unix socket进行通讯，但是unix socket属于root用户，但是普通用户需要使用sudo才能开启root权限，但是普通的操作并没有root权限。 Ubuntu: sudo usermod -aG docker $&#123;USER&#125; Mac: cmd-shift-p 点击 &quot;Preferences: Open Workspace Settings&quot; 搜索 “docker path” 输入docker client的绝对路径 (通常在 &quot;/usr/local/bin/docker&quot;) Windows: 同 Mac Windows WSL: 暂未解决","tags":["Docker"],"categories":["解决方案"]},{"title":"Golang Module 换国内镜像","path":"/2024/01/30/2024-01-30-golang-set-goproxy/","content":"Golang 很多包在国内下载很慢，换国内镜像可以加快下载速度。 启用 Go Modules 功能 go env -w GO111MODULE=on 配置 GOPROXY 环境变量，以下三选一 1. 七牛 CDN go env -w GOPROXY=https://goproxy.cn,direct 2. 阿里云 go env -w GOPROXY=https://mirrors.aliyun.com/goproxy/,direct 3. 官方 go env -w GOPROXY=https://goproxy.io,direct 确认是否已更新 12go env | grep GOPROXYGOPROXY=&quot;https://goproxy.cn&quot;","tags":["Golang"],"categories":["技术加油站"]},{"title":"相似度计算: Cosine Similarity","path":"/2023/06/28/2023-07-02-similarity-measurement/","content":"相似度计算在数据挖掘中指的是:计算在数据集里使用多个维度数据来表示的object 的距离. 一些常用的受欢迎的相似度测量方法有: Euclidean Distance. 欧几里得距离(euclidean distance) 在二维和三维空间中的欧氏距离就是两点之间的实际距离。 Manhattan Distance. 在 二维空间 内，两个点之间的曼哈顿距离为它们横坐标之差的绝对值与纵坐标之差的绝对值之和。两个点的曼哈顿距离是会随着坐标系的改变而改变的. Minkowski Distance. 闵科夫斯基距离是规范化向量空间（N维实空间）中两点之间的距离/相似性，是欧氏距离和曼哈顿距离的概括。 相似度一般都是基于某种距离的定义来计算的. 余弦相似度 (Cosine Similarity). 余弦相似度是一种度量，有助于确定数据对象的相似程度，无论其大小如何。在余弦相似度的计算中,所有的数据集都被认为是向量(vector, 通常是 1xN的 shape). 求解相似度的公式是: 比如我们有两个向量: x = {3,2,0,5} &amp; y = {1, 0, 0, 0} 其相似度的计算就是: 1234567891011x . y = 3*1 + 2*0 + 0*0 + 5*0 = 3||x|| = √ (3)^2 + (2)^2 + (0)^2 + (5)^2 = 6.16||y|| = √ (1)^2 + (0)^2 + (0)^2 + (0)^2 = 1Cos(x, y) = 3 / (6.16 * 1) = 0.49 # 两个向量的差异度就是用 100% 减去相似度.Dis(x, y) = 1 - Cos(x, y) = 1 - 0.49 = 0.51 扩展:计算句子的相似度 Sentence Encode 计算句子的相似度之前,需要使用多维向量来表示一个句子. 一般 NLP 领域都有一个 tokenizer 来加密句子. 在这个例子中我们使用了常用的NLTK :: Natural Language Toolkit : nltk. 12345678910111213141516171819202122232425262728293031323334353637# Program to measure the similarity between# two sentences using cosine similarity.from nltk.corpus import stopwordsfrom nltk.tokenize import word_tokenize# X = input(\"Enter first string: \").lower()# Y = input(\"Enter second string: \").lower()X =\"I love horror movies\"Y =\"Lights out is a horror movie\"# tokenizationX_list = word_tokenize(X)Y_list = word_tokenize(Y)# sw contains the list of stopwordssw = stopwords.words('english')l1 =[];l2 =[]# remove stop words from the stringX_set = {w for w in X_list if not w in sw}Y_set = {w for w in Y_list if not w in sw}# form a set containing keywords of both stringsrvector = X_set.union(Y_set)for w in rvector:\tif w in X_set: l1.append(1) # create a vector\telse: l1.append(0)\tif w in Y_set: l2.append(1)\telse: l2.append(0)c = 0# cosine formulafor i in range(len(rvector)): c+= l1[i]*l2[i]cosine = c / float((sum(l1)*sum(l2))**0.5)print(\"similarity: \", cosine) 优点: 余弦相似度是很有用的，因为即使两个相似的数据对象由于大小而相距欧几里得距离很远，它们之间仍然可以具有较小的角度。角度越小，相似度越高。 当在多维空间上绘制时，余弦相似度捕获数据对象的方向（角度）从而减少幅度的影响。","tags":["Python","Pandas","Data Science"],"categories":["技术加油站"]},{"title":"[Debug] Numpy Array 不显示全部维度的 shape","path":"/2023/06/25/2023-06-25-ndarray-not-show-all-dimensions/","content":"前一段时间发现从一个 DataFrame 中提取出一个Series. 不管被提取的 Series 的每一个值是不是一个 ndarray,返回值永远是一个一维的 ndarray. 发现问题 比如有一个 Dataframe 1234# each label_embed_vector contains 100 vectors. img_key ... label_embed_vector0 1628112148035278_1_31 ... 0.006821885239,-0.068260461092,-0.010561314411...1 1647947462308801_2_622 ... 0.028957819566,-0.027704223990,-0.001513824915... 直接用 df['label_embed_vector']来读取数据的话,返回的是一个小的 dataframe. 这个时候就需要使用 Dataframe.values 来读取数据. 这个方法会返回一个包含数据的 numpy array. 但是这个 ndarry 只是 1 维的. 在pandas.DataFrame.values的官方文档里,说推荐pandas.DataFrame.to_numpy 来代替 values 方法. 但是使用推荐的方法以后返回的结果还是 1 维的: 123456789# 使用`to_numpy`方法来读取`label_embed_vector`的数据以后发现他返回的是一个 shape 为 (86,)的 ndarray.data = df[&#x27;label_embed_vector&#x27;].to_numpy()print(data.shape)&gt;&gt;&gt; (86,)# 读取第一个数据print(data[0])&gt;&gt;&gt; list([0.006821885239,-0.068260461092,-0.010561314411...]) 上面的输出很奇怪, 从读取的数据来看的话, data 是一个(86,100)的 ndarray. 但是 data 的 shape 却是(86,). 原因和解决方案 通过 print 来检查数据以后发现, 虽然数据结构类似而且读取数据的时候和多维数组一样, 不过输出的数组确实不是多维度的: 12345# printout 检查数据print(df[&#x27;label_embed_vector&#x27;].to_numpy())&gt;&gt;&gt; array([arrar([0.006821885239,-0.068260461092,-0.010561314411...], array([0.028957819566,-0.027704223990,-0.001513824915..], ... , array([-0.000122768164,-0.020344628021,0.018706960604...])]# .values 和 to_numpy 返回的是 1个包含了 86 个独立 array 的 1 维array.而不是多维的. 解决方案 经过搜索发现 Pandas 这样做的原因是不能确定每一个 row 的 array 的长度是一样的, 因此返回的是一维数据. 这个时候可以用np.vstack来手动把一位数据来转化成多维数据. 比如: 12data = np.vstack(df[&#x27;label_embed_vector&#x27;].to_numpy())","tags":["Python","Pandas","Data Science"],"categories":["技术加油站"]},{"title":"在 M1 Mac上搭建 Pytorch GPU 加速环境","path":"/2023/02/23/2023-02-23-setup-pytorch-gpu-on-m1-mac/","content":"M1 Max 的芯片的 GPU 算力已经很不错了, 想着自己不是视频编辑工作者,总不能这么浪费这么好的资源.偶尔用这电脑来跑跑中小型的模型也是不错的选择. TensorFlow 到现在对 M1 的支持还是有点一塌糊涂的感觉. 知名的 YOLO 模型也基本只有 PyTorch 版本. 想想 PyTorch 就是数据科学家必须了解的知识点. 我所用的云服务器基本都在外网,由于众所周知的因素,链接很慢也不是很稳定. 测试学习之类的还是在本地比较好. 安装 PyTorch 下面是用来安装的命令以及注释: 如果你不确定这是不是你需要的版本, 请移步到PyTorch 官网来查看官网的文档. 123456789101112# 官网推荐安装 python 3.7或以上版本.# 由于其他包的关系,这里推荐 3.8 的最新版conda create -n torch-gpu python=3.8conda activate torch-gpu# 安装 PyTorch. MPS acceleration is available on MacOS 12.3+conda install pytorch torchvision torchaudio -c pytorch# 安装其他工具或依赖conda install -c conda-forge jupyter jupyterlabconda install numpyconda install matplotlibconda install pandas 测试 如果是这种输出,那么恭喜你,安装完成. 1234python&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.backends.mps.is_available()True","tags":["PyTorch","Metal"],"categories":["解决方案"]},{"title":"如果启用 Hexo 博客的数学公式支持","path":"/2023/02/03/2023-02-03-hexo-set-up-math-support/","content":"Hexo默认的markdown渲染器是不支持数学公式的输入的. 一般来说都会用hexo的一些插件来实现对数学公式的支持. 这里数学公式的输入一般都是用的LaTeX. 安装 hexo-renderer-pandoc 因为Hexo默认的渲染插件对数学公式不支持,我们需要把默认的markdown渲染插件先移出,然后再安装我们所需要的插件: hexo-renderer-pandoc 12345# 卸载Hexo默认渲染插件：npm uninstall hexo-renderer-marked --save# 安装pandoc渲染插件：npm install hexo-renderer-pandoc --save 安装 mathjax 这一步对于某些运行环境来说不是必须的. 但是如果你安装pandoc以后还是没有成功渲染出来数学公式的话,那么就需要安装mathjax了. 1npm install hexo-filter-mathjax --save 配置 config 文件 在hexo 的配置文件: _config.yml 中添加以下代码: 12345678math: ... mathjax: enable: true # Next v6.3.0 后的版本 tags 要设置成 ams # Available values: none | ams | all tags: ams 清除缓存并重新构建文件 12hexo cleanhexo generate 部署或者使用本地服务测试 12345# 使用本地服务测试hexo s# 部署hexo deploy 使用数学公式的页面 Deep Learning Intro","tags":["Hexo","LaTeX"],"categories":["解决方案"]},{"title":"计算机视觉中的数据增强","path":"/2022/06/21/2022-06-21-cv-data-augmentation/","content":"数据增强是比较机器学习中比较常用的方法. 但是并不是所有的数据增强都可以有效的提升精度. 有些数据增强的方法很可能会增加噪声,使模型表现比数据增强之前还要差. 什么是数据增强 数据增强（Data Augmentation）是一种通过让有限的数据产生更多的等价数据来人工扩展训练数据集的技术。它是克服训练数据不足的有效手段，目前在深度学习的各个领域中应用广泛。但是由于生成的数据与真实数据之间的差异，也不可避免地带来了噪声问题。 为什么需要数据增强 深度神经网络在许多任务中表现良好，但这些网络通常需要大量数据才能避免过度拟合。遗憾的是，许多场景无法获得大量数据，例如医学图像分析。数据增强技术的存在是为了解决这个问题，这是针对有限数据问题的解决方案。数据增强一套技术，可提高训练数据集的大小和质量，以便您可以使用它们来构建更好的深度学习模型。在计算视觉领域，生成增强图像相对容易。即使引入噪声或裁剪图像的一部分，模型仍可以对图像进行分类，数据增强有一系列简单有效的方法可供选择，有一些机器学习库来进行计算视觉领域的数据增强，比如：它封装了很多数据增强算法，给开发者提供了方便。 CV中常见的图像增强的方法 CV领域的数据增强大致可以分为两类: 第一类是基于基本图像处理技术的数据增强，第二个类别是基于深度学习的数据增强算法. 基于图像处理的数据增强方法 Flipping (翻转) 一般都是水平方向翻转,很少用垂直方向(镜像变换). Rotation (旋转) 一般使用旋转的时候,需要注意旋转角度. 以数据集MNIST为例, 轻微旋转(1-20°)可能有用. Color Space (色彩空间) 简单做法是隔离单个色彩通道，例如Red®，Green(G)或Blue(B)，此外可以通过简单的矩阵运算以增加或减少图像的亮度。更高级的做法从颜色直方图着手，更改这些直方图中的强度值(想到了图像处理中的直方图均衡)。一般可以调节的有对比度(contrast),亮度(brighteness),色调(hue) Cropping (裁剪) 分统一裁剪和随机裁剪。统一裁剪将不同尺寸的图像裁剪至设定大小，随机裁剪类似translation，不同之处在于translation保留原图尺寸而裁剪会降低尺寸。裁剪要注意不要丢失重要信息以至于改变图像标签。 eg. ssd_crop (详解?) translation(位置变换) 向左，向右，向上或向下移动图像可能是非常有用的转换，以避免数据中的位置偏差。例如人脸识别数据集中人脸基本位于图像正中，位置变换可以增强模型泛化能力。 noise injection(添加噪声) color space transformations(色彩空间增强) 照明偏差是图像识别问题中最常见的挑战之一，因此色彩空间转换（也称为光度转换）的比较直观有效。 遍历图像以恒定值减少或增加像素值（过亮或过暗） 拼接出（splice out）各个RGB颜色矩阵 将像素值限制为某个最小值或最大值 操作色彩直方图以改变图像色彩空间特征 注意将彩色图转换黑白虽然简化了这些操作，但精度会降低 geometric versus photometric transformations(几何与光度转换) todo 基于深度学习的数据增强算法 mixing images(图像混合) 做法是通过平均图像像素值将图像混合在一起: 研究发现是当混合来自整个训练集的图像而不是仅来自同一类别的实例的图像时，可以获得更好的结果。其它一些做法： 一种线性方法是将图像组合成新的训练实例: 另一方法是随机裁剪图像并将裁剪后的图像连接在一起以形成新图像: Random Crop then montage 随机裁剪再拼接 这类方法从人的视角看毫无意义，但确实提升了精度。可能解释是数据集大小的增加导致了诸如线和边之类的低级特征的更可靠表示。 random erasing随机擦除 这一点受到dropout正规化的启发，随机擦除迫使模型学习有关图像的更多描述性特征，从而防止过拟合某个特定视觉特征。随机擦除的好处在于可以确保网络关注整个图像，而不只是其中的一部分。随机擦除一般分为(1)Image-ware random Erasing(2) Object-ware Random Erasing 和 (3)Image and Object-ware Random Erasing. 最后随机擦除的一个缺点是不一定会保留标签. KeepAugment (保持增强) 新方法控制数据增强的保真度，从而减少有害的错误信息。研究者的想法是通过显著性映射测量图像中矩形区域的重要性，并确保数据增强后始终呈现得分最高的区域：对于裁剪，通过避免切割重要区域(见下图a5和b5);对于图像级转换，通过将重要区域粘贴到转换图像顶部 KeepAugment的伪代码: 如何选择数据增强 数据增强需要根据数据集而定。不过一般的像图像旋转，图像翻转等方法比较通用，在任何数据集上都有良好的表现。","tags":["Computer Vision","Machine Learning","Deep Learning"],"categories":["技术加油站"]},{"title":"优化物体检测结果时常用的集成方法","path":"/2022/06/10/2022-06-16-nms-vs-softnms-vs-wbf/","content":"物体检测中一般模型会在一个物体上检测出多个拥有不同confidence score的格子, 但是对于输出结果来说,我们并不需要那么多的预测结果.因此,我们会用一些集成方法(ensemble method)来把这些预测结果合并成一个. Non-maximum Suppression (NMS, 非最大化抑制) NMS的思路是对所有类别的检测框进行循环过滤.对于某个类别, NMS 会根据预测结果的confidence score来把预测结果排序,然后选择有最大confidence的预测结果. 在此之后,通过一个IOU(Intersection over Union) 的threshold,就可以过滤其他的预测结果从而减少预测结果的重复性. 基于这种计算逻辑的NMS有两个缺点。首先，NMS算法需要一个超参即IOU Threshold，这个阈值在不同任务中很难平衡。其次，NMS会将相邻或者重叠的两个物体对应的两个大概率目标框去掉一个，造成漏检。 Soft-NMS Soft NMS方法总流程和NMS算法流程相同. Soft NMS算法会根据IOU成比例的减少confidence score的值. 对于重叠的框,重叠区(IOU)越大,置信度衰减越严重. 一般的, Soft NMS降低置信度权重的计算方法有两个: 1. 线性法 2. 高斯法 NMS是一种特殊的线性法的Soft-NMS.(如下) $$ s_i = \\left. \\begin{cases} s_i, iou &lt; T \\ 0, iou \\ge T\\ \\end{cases} \\right. $$ 计算方法最好选用连续性的函数,因为采用不连续的函数会导致box集合中的confidence score出现断层. WBF (Weighted Boxes Fusion) method 因为NMS和Soft-NMS会删掉很多的检测结果,所以可以说这些集成方法并没有用到模型预测出来的所有信息. 而WBF使用所有预测结果的置信度来计算出一个平均的检测结果,所以WBF可以在所有结果都不太准确的时候修正结果. NMW (Non-maximum Weighted) method NMW方法与WBF方法使用了相似的思路，只是NMW方法使用了IoU来调整检测结果的权重。","tags":["Computer Vision","Big Data"],"categories":["技术加油站"]},{"title":"高级数据结构 SSTable","path":"/2022/05/18/2022-05-18-upper-level-datatype-sstable/","content":"Sorted String Table(SSTable)–是存储，处理和交换数据集的最流行的输出之一。正如名字本身所包含的意思一样，SSTable是一个简单的抽象，用来高效地存储大量的键-值对数据，同时做了优化来实现顺序读/写操作的高吞吐量。 什么是 SSTable？ Google 发布的Bigtable论文中解释道：SSTable提供一个可持久化[persistent]，有序的、不可变的从键到值的映射关系，其中键和值都是任意字节长度的字符串。SSTable提供了以下操作：按照某个键来查询关联值，可以指定键的范围，来遍历其中所有的键值对。每个SSTable内部由一系列块(block)组成(通常每块大小为64KB，是可配置的)。使用存储在SSTable结尾的块索引(block index)来定位块；当SSTable打开时，索引会被加载到内存里。一次磁盘寻道(disk seek)就可以完成查询(lookup)操作：首先通过二分查找在存储在内存的索引中找到对应的块，然后从磁盘上读取这块内容。SSTable也可以完整地映射到内存里，这样在执行查询和扫描(scan)的时候就不用操作磁盘了. SSTable 结构 Scylladb在其官网(What is a SSTable? Definition &amp; FAQs | ScyllaDB)中做出了以下解释： Sorted Strings Table (SSTable) is a persistent file format used by ScyllaDB, Apache Cassandra, and other NoSQL databases to take the in-memory data stored in memtables, order it for fast access, and store it on disk in a persistent, ordered, immutable set of files. Immutable means SSTables are never modified. They are later merged into new SSTables or deleted as data is updated. 其实除了Scylladb以外，当有很多公司使用SSTable，这些SSTable有些许轻微的不同。这里我们只研究BigTable中的SSTable。 什么是memtable？MemTable是一种在内存中保存数据的数据结构，然后再在合适的时机，MemTable中的数据会flush到SST file中。MemTable既可以支持读服务也可以支持写服务，写操作会首先将数据写入Memtable，读操作在query SST files之前会首先从MemTable中query数据（因为MemTable中的数据一直是最新的）。一旦MemTable满了，就会转换为只读的不可改变的，然后会创建一个新的MemTable来提供新的写操作。后台线程负责将MemTable中的数据flush到SST file，然后这个MemTable就会被销毁。 SSTable 有何作用？ 01：储存格式 在新数据写入时，这个操作首先提交到日志中作为redo纪录，最近的数据存储在内存的排序缓存memtable中；旧的数据存储在一系列的SSTable 中。在recover中，tablet server从METADATA表中读取metadata，metadata包含了组成Tablet的所有SSTable（纪录了这些SSTable的元 数据信息，如SSTable的位置、StartKey、EndKey等）以及一系列日志中的redo点。Tablet Server读取SSTable的索引到内存，并replay这些redo点之后的更新来重构memtable。在读时，完成格式、授权等检查后，读会同时读取SSTable、memtable（HBase中还包含了BlockCache中的数据）并合并他们的结果，由于SSTable和memtable都是字典序排列，因而合并操作可以很高效完成。 02：压缩（Compaction）过程中的使用 随着memtable大小增加到一个阀值，这个memtable会被冻住而创建一个新的memtable以供使用，而旧的memtable会转换成一个SSTable而写道GFS中，这个过程叫做minor compaction。这个minor compaction可以减少内存使用量，并可以减少日志大小，因为持久化后的数据可以从日志中删除。在minor compaction过程中，可以继续处理读写请求。 每次minor compaction会生成新的SSTable文件，如果SSTable文件数量增加，则会影响读的性能，因而每次读都需要读取所有SSTable文件，然后合并结果，因而对SSTable文件个数需要有上限，并且时不时的需要在后台做merging compaction，这个merging compaction读取一些SSTable文件和memtable的内容，并将他们合并写入一个新的SSTable中。当这个过程完成后，这些源SSTable和memtable就可以被删除了。 如果一个merging compaction是合并所有SSTable到一个SSTable，则这个过程称做major compaction。一次major compaction会将mark成删除的信息、数据删除，而其他两次compaction则会保留这些信息、数据（mark的形式）。Bigtable会时不时的扫描所有的Tablet，并对它们做major compaction。这个major compaction可以将需要删除的数据真正的删除从而节省空间，并保持系统一致性。 SSTable 设计成immutable的优点 在读SSTable是不需要同步。读写同步只需要在memtable中处理，为了减少memtable的读写竞争，Bigtable将memtable的row设计成copy-on-write，从而读写可以同时进行。 永久的移除数据转变为SSTable的Garbage Collect。每个Tablet中的SSTable在METADATA表中有注册，master使用mark-and-sweep算法将SSTable在GC过程中移除。 可以让Tablet Split过程变的高效，我们不需要为每个子Tablet创建新的SSTable，而是可以共享父Tablet的SSTable。 更多 《Bigtable: A Distributed Storage System for Structured Data》 《Bigtable: A Distributed Storage System for Structured Data》论文翻译","tags":["BigData"],"categories":["技术加油站"]},{"title":"图像分割与图像检测","path":"/2022/04/24/2022-04-24-img-segmentation/","content":"计算机视觉可以分为两大方向：基于学习的方法和基于几何的方法。其中基于学习的方法最火的就是深度学习，而基于几何方法最火的就是视觉SLAM(Simultaneous Localization and Mapping) 图像分割 图像分割：图像分割就是把图像分成若干个特定的、具有独特性质的区域并提出感兴趣目标的技术和过程。 它是由图像处理到图像分析的关键步骤。图形分割是Target Segmentation，是data/image segmentation的一种。任务是把目标对应的部分分割出来。对于一般的光学图像而言，分割像素是一个比较常见的目标，就是要提取哪一些像素是用于表述已知目标的。这种Segmentation可以是一个分类（classificatio）问题，就是把每一个pixel做labeling，提出感兴趣的那一类label的像素。也可以是clustering的问题，即是不知道label，但需要满足一些optimality，比如要cluster之间的correlation最小之类的。（像素级的对前景与背景进行分类，将背景剔除） 图像分割 目标检测 目标检测：目标检测是Target Detection。最早的detection system应该是搞雷达的人首先提出并且heavily study的，最简单的任务就是从看似随机（random）又充满干扰（interference）和噪音（noise）的信号中，抓取到有信息的特征（information-bearing pattern）。最简单的一个栗子，就是当你拿到一段随机的雷达回波，可以设置一个threshold，当高于这个threshold，就认为是探测到了高速大面积飞行器之类的高回波的目标。当然，这里面的threshold该怎么设计，涉及到False Alarm和Miss Detection之间的平衡。人们往往需要寻找最佳的transform或者domain去对信号进行分析。（定位目标，确定目标位置及大小） 图像检测 从输入和输出来比较的话,物体检测的输入和图像分割的输入都是一样的. 一般情况下都是x * y * 3的数组(可以理解为RGB各一层)或者是x * y的 binary image. 物体检测的输出一般都是一个bounding box的列表,而图像分割的输出是一个矩阵, 用来表示mask. 分割和检测融合? 目前也有很多算法将分割和检测进行融合：比如: RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation Joint 3D Instance Segmentation and Object Detection for Autonomous Driving Gated-SCNN: Gated Shape CNNs for Semantic Segmentation etc. 通过一个/一种深度学习的结构是很难把这两种不同的任务结合到一起的,以Gated-SCNN: Gated Shape CNNs for Semantic Segmentation为例, 作者认为通过一个深度CNN网络同时处理图像的颜色，形状和纹理信息用于像素级分类可能不是理想的做法，因此该论文提出two-stream CNN结构，将形状信息作为单独处理的分支。两分支包括shape stream和regular stream，二者相互协作且是并行的，通过设计的门控卷积层和局部监督损失有效地移除了噪声并帮助shape stream仅仅处理与边界相关的信息，然后通过ASPP模块保留多尺度上下文信息，融合regular stream的语义区域特征和shape stream的边界特征产生refined的分割输出，尤其refine在边界周围的像素分割。","tags":["Computer Vision"],"categories":["技术加油站"]},{"title":"Bash中的比较语法详解","path":"/2022/04/05/2022-04-04-bash_eq_talk/","content":"如果你希望通过脚本运算获得的结果是你想要的，那么你需要应用到算术运算符参与算术运算以获得你所需的结果. 在Bash脚本中,一般比较语法都会放在方括号中. 单括号与双括号的对比 属性 单括号 [ ] 双括号 [[ ]] 括号两边都要有空格分隔 √ √ 内部操作符与变量之间要有空格 √ √ 字符串比较时需要在&lt; &gt;之前加入\\进行转义 √ 括号中字符串或者${}变量尽量使用”” 双引号扩住 √ √ 括号中如若未使用&quot;&quot;会进行模式和元字符匹配 √ 括号中可以使用-a或者-o等进行逻辑运算 √ 括号中可以使用&amp;&amp;或者||进行逻辑运算 √ 括号可以配合!使用 √ √ 此外, [ ] 是bash 内置命令 而**[[ ]] 是bash 关键词, 双方括号表达式会返回一个退出码**. ==与-eq的对比以及延伸 一般来说, 使用 =或者==来做字符串的对比; 使用-eq来做数值的对比. 使用==来对比字符串 str命令 含义 [[ str ]] 判断字符串是否不为空 [[ -z str ]] 判断字符串是否为空, [[ -n str ]] 判断字符串是否不为空 [[ str1 = str2 ]] 判断字符串是否相同 [[ str1 == str2 ]] 判断字符串是否相同 [[ str1 != str2 ]] 判断字符串是否不相等 [[ str1 \\\\&lt; str2 ]] 判断按字典序排序str1是否在str2之前 [[ str1 \\\\&gt; str2 ]] 判断按字典序排序str1是否在str2之后 使用 -eq 等命令来对比数值 num命令 含义 [[ val1 -eq val2 ]] 判断val1是否与val2相等 [[ val1 -ne val2 ]] 判断val1是否与val2不相等 [[ val1 -lt val2 ]] 判断val1是否小于val2 [[ val1 -le val2 ]] 判断val1是否小于等于val2 [[ val1 -gt val2 ]] 判断val1是否大于val2 [[ val1 -ge val2 ]] 判断val1是否大于等于val2 文件状态检查 命令 含义 -b filename 检查是否文件存在并且是块文件* -c filename 检查是否文件存在并且是字符文件 -d path 检查是否文件存在并且是目录 -f filename 检查是否文件存在并且为正规文件 -g path 检查是否文件存在并且设置了SGID -h filename 检查是否文件存在并且是符号链接文件 -p filename 见怕是否文件存在并且是命名管道 -s filename 检查是否文件存在并且文件大小大于0 -S filename 检查是否文件存在并且是socket file1 -nt file2 检查是否file1比file2新 file1 -ot file2 检查是否file1比file2旧 file1 -ef file2 检查是否file1和file2 链接到了相同的文件 块存储的用户是可以读写块设备的软件系统. 文件存储的用户是自然人. 对象储存的用户是其他计算机软件. Linux系统中所有文件/文件夹都是已文件的格式保存的. 但是为了更容易区分.文件和文件/文件夹不同的命令区分开来列出. 命令 含义 -u path 检查是否path制定的文件或目录存在并且设置了SUID -w path 检查是否path制定的文件或目录存在并且可写 -x path 检查是否path指定的文件或目录存在并且可执行 -r path 检查是否文件/目录存在并且可读 -e filename 检查是否文件/目录存在 -O path 检查是否path存在并且被当前进程的有效用户id 的用户拥有 -G path 检查是否path 存在并且属于当前进程的有效用户id 的用户的用户组 双括号 [[ ]] 对比单括号 [ ] 的优势 [[是 bash 程序语言的关键字。并不是一个命令，[[ ]] 结构比[ ]结构更加通用。在[[和]]之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换。 支持字符串的模式匹配，使用=~操作符时甚至支持shell的正则表达式。字符串比较时可以把右边的作为一个模式，而不仅仅是一个字符串，比如[[ hello == hell? ]]，结果为真。[[ ]] 中匹配字符串或通配符，不需要引号。 使用[[ … ]]条件判断结构，而不是[… ]，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中，但是如果出现在[ ]结构中的话，会报错。 bash把双中括号中的表达式看作一个单独的元素，并返回一个退出状态码。 使用[[ … ]]条件判断结构, 而不是[ … ], 能够防止脚本中的许多逻辑错误. 比如,&amp;&amp;, ||, &lt;, 和&gt; 操作符能够正常存在于[[]]条件判断结构中, 但是如果出现在[ ]结构中的话, 会报错. 因此推荐直接选用双括号.","tags":["Bash"],"categories":["技术加油站"]},{"title":"稳定婚姻算法","path":"/2022/04/04/2022-04-04-stable-marriage/","content":"“稳定婚姻问题”在生活中是一个典型的问题,通俗地可叙述为：当前有N位男生和N位女生最后要组成稳定的婚姻家庭，过程开始之前男生和女生在各自的心目中都按照喜爱程度对N位异性有了各自的排序，男生和女生结婚后，对于每一对男生女生，不会出现比起当前匹配的伴侣互相更喜爱的一对男生女生，即可认为婚姻是稳定的。 1962 年，美国数学家 David Gale 和 Lloyd Shapley 发明了一种寻找稳定婚姻的策略。不管男女各有多少人，不管他们各自的偏好如何，应用这种策略后总能得到一个稳定的婚姻搭配。换句话说，他们证明了稳定的婚姻搭配总是存在的。 解决算法 先对所有男士进行落选标记，称其为自由男。当存在自由男时，进行以下操作： 每一位自由男在所有尚未拒绝她的女士中选择一位被他排名最优先的女士 每一位女士将正在追求她的自由男与其当前男友进行比较，选择其中排名优先的男士作为其男友，即若自由男优于当前男友，则抛弃前男友；否则保留其男友，拒绝自由男。 若某男士被其女友抛弃，重新变成自由男。 在算法执行期间，自由男们主动出击，依次对最喜欢和次喜欢的女人求爱，一旦被接受，即失去自由身，进入订婚状态；而女人们则采取“守株待兔”和“喜新厌旧”策略，对前来求爱的男士进行选择：若该男子比未婚夫强，则悔婚，选择新的未婚夫；否则拒绝该男子的求婚。被女友抛弃的男人重获自由身，重新拥有了追求女人的权利——当然，新的追求对象比不过前女友。 这样，在算法执行期间，每个人都有可能订婚多次——也有可能一开始就找到了自己的最爱，从一而终——每订一次婚，女人们的选择就会更有利，而男人们的品味则越来越差。只要男女生的数量相等，则经过多轮求婚，订婚，悔婚和再订婚之后，每位男女最终都会找到合适的伴侣——虽然不一定是自己的最爱（男人没能追到自己的最爱，或女人没有等到自己的最爱来追求），但绝对不会出现“虽然彼此相爱，却不能在一起”的悲剧，所有人都会组成稳定的婚姻。 示例代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# Python3 program for stable marriage problem# Number of Men or WomenN = 4# This function returns true if# woman &#x27;w&#x27; prefers man &#x27;m1&#x27; over man &#x27;m&#x27;def wPrefersM1OverM(prefer, w, m, m1): # Check if w prefers m over her # current engagement m1 for i in range(N): # If m1 comes before m in list of w, # then w prefers her current engagement, # don&#x27;t do anything if (prefer[w][i] == m1): return True # If m comes before m1 in w&#x27;s list, # then free her current engagement # and engage her with m if (prefer[w][i] == m): return False# Prints stable matching for N boys and N girls.# Boys are numbered as 0 to N-1.# Girls are numbered as N to 2N-1.def stableMarriage(prefer): # Stores partner of women. This is our output # array that stores passing information. # The value of wPartner[i] indicates the partner # assigned to woman N+i. Note that the woman numbers # between N and 2*N-1. The value -1 indicates # that (N+i)&#x27;th woman is free wPartner = [-1 for i in range(N)] # An array to store availability of men. # If mFree[i] is false, then man &#x27;i&#x27; is free, # otherwise engaged. mFree = [False for i in range(N)] freeCount = N # While there are free men while (freeCount &gt; 0): # Pick the first free man (we could pick any) m = 0 while (m &lt; N): if (mFree[m] == False): break m += 1 # One by one go to all women according to # m&#x27;s preferences. Here m is the picked free man i = 0 while i &lt; N and mFree[m] == False: w = prefer[m][i] # The woman of preference is free, # w and m become partners (Note that # the partnership maybe changed later). # So we can say they are engaged not married if (wPartner[w - N] == -1): wPartner[w - N] = m mFree[m] = True freeCount -= 1 else: # If w is not free # Find current engagement of w m1 = wPartner[w - N] # If w prefers m over her current engagement m1, # then break the engagement between w and m1 and # engage m with w. if (wPrefersM1OverM(prefer, w, m, m1) == False): wPartner[w - N] = m mFree[m] = True mFree[m1] = False i += 1 # End of Else # End of the for loop that goes # to all women in m&#x27;s list # End of main while loop # Print solution print(&quot;Woman &quot;, &quot; Man&quot;) for i in range(N): print(i + N, &quot;\\t&quot;, wPartner[i])# Driver Codeprefer = [[7, 5, 6, 4], [5, 4, 6, 7], [4, 5, 6, 7], [4, 5, 6, 7], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]stableMarriage(prefer)# This code is contributed by Mohit Kumar 数学问题的现实应用价值 现实中的婚姻问题比这个模型要复杂的多, 不过这个模型的思想可以用来在实际社会中的匹配任务. 比如医学生毕业时的工作分配.","tags":["Algorithm"],"categories":["技术加油站"]},{"title":"常见的图片格式概览","path":"/2022/03/24/2022-05-30-common-images-format/","content":"数据都是以二进制的格式存在计算机中。图片作为一种数据，当然也是以二进制的格式存在计算机中。此篇不讲图片数据格式的底层，只是概述一下各个图片格式的特点和优缺点。 常见的图片格式 一般的，生活中常见、常用的图片格式有png，jpeg，gif，webp。工作中偶尔会用的有BMP，TIF，svg。各个格式有各个格式的优缺点，目前没有一种统一的图片格式可以替代所有。GIF/PNG/JPG/WEBP/APNG等都是属于位图，而SVG是矢量图。 这里提一下PDF格式。很多人错误的以为PDF是图片的格式，但是PDF的全称是PortableDocument Format，意为&quot;可携带文档格式”。是由Adobe Systems用于与应用程序、操作系统、硬件无关的方式进行文件交换所发展出的文件格式。PDF文件以PostScript语言图象模型为基础，使其在各型号打印机上都可再现原稿的字符、颜色以及图象信息。简单的说，PDF除了有图片信息（图片）以外，还有一些肉眼看不到的信息和metadata。 各种图片的优缺点 WebP WebP 的优势体现在它具有更优的图像数据压缩算法，能带来更小的图片体积，而且拥有肉眼识别无差异的图像质量；此外他还具备了无损和有损的压缩模式、Alpha 透明以及动画的特性，在 JPEG 和 PNG 上的转化效果都相当优秀、稳定和统一。WebP支持的像素最大数量是16383x16383。iSparta做了一个PNG图片和WebP图片压缩和不压缩的质量对比和大小对比。点此查看. WebP有静态与动态两种模式。动态WebP（Animated WebP）支持有损与无损压缩、ICC色彩配置、XMP诠释资料、Alpha透明通道。相对于GIF，WebP格式有近乎相同的图片质量和相当小的数据大小。通过官方给出的例子我们可以明显的看出WebP的优点。缺点是你压缩的时候需要的时间更久了，而且使用WebP的时候CPU的负载更多。 GIF（Graphics Interchange Format） GIF图形交换格式是一种位图图形文件格式,以8位色（即256种颜色）重现真彩色的图像。它实际上是一种压缩文档,采用LZW压缩算法进行编码,有效地减少了图像文件在网络上传输的时间。它有很多优点，比如： 优秀的压缩算法使其在一定程度上保证图像质量的同时将体积变得很小。 可插入多帧,从而实现动画效果。 可设置透明色以产生对象浮现于背景之上的效果。 但是由于采用了8位压缩,最多只能处理256种颜色（2的8次方）,故不宜应用于真彩图像。 PNG（Portable Network Graphics） 便携式网络图片（Portable Network Graphics）,简称PNG,是一种无损数据压缩位图图形文件格式。PNG格式是无损数据压缩的,PNG格式有8位、24位、32位三种形式,其中8位PNG支持两种不同 的透明形式（索引透明和alpha透明）,24位PNG不支持透明,32位 PNG 在24位基础上增加了8位透明通道（32-24===8）,因此可展现256级透明程度。 PNG这种类型的图片就是为了取代GIF图片而生的, 除了GIF不支持动画的优势, 能用PNG的地方就用PNG, 原因是压缩比高,色彩好； 相比较GIF图片，PNG图片的优点更多： 支持256色调色板技术以产生小体积文件 最高支持48位真彩色图像以及16位灰度图像。 支持Alpha通道的半透明特性。 支持图像亮度的gamma校正信息。 支持存储附加文本信息,以保留图像名称、作者、版权、创作时间、注释等信息。 使用无损压缩。 渐近显示和流式读写,适合在网络传输中快速显示预览效果后再展示全貌。 使用CRC循环冗余编码防止文件出错。 最新的PNG标准允许在一个文件内存储多幅图像。 JPG（Joint Photographic Experts Group） JPEG是一种针对相片影像而广泛使用的一种失真压缩标准方法。JPEG的压缩方式通常是破坏性资料压缩（lossy compression）,意即在压缩过程中图像的品质会遭受到可见的破坏。 JPEG/JFIF是最普遍在万维网（World Wide Web）上被用来储存和传输照片的格式。JPG压缩算法将不易被人眼察觉的图像颜色删除，从而达到较大的压缩比（可达到2:1甚至40:1），可以说是用最少的磁盘空间得到较好的图像质量。JPEG在色调及颜色平滑变化的相片或是写实绘画（painting）上可以达到它最佳的效果。在这种情况下,它通常比完全无失真方法作得更好,仍然可以产生非常好看的影像（事实上它会比其他一般的方法像是GIF产生更高品质的影像,因为GIF对于线条绘画（drawing）和图示的图形是无失真,但针对全彩影像则需要极困难的量化)。但是，它并不适合于线条绘图（drawing）和其他文字或图示（iconic）的图形,因为它的压缩方法用在这些图形的型态上,会得到不适当的结果； 更多关于JPEG的详细解释，请参考这篇文章 BMP 位图（Bitmap）格式的数据没有经过压缩，或最多只采用行程长度编码（RLE，run-length encoding）来进行轻度的无损数据压缩。以至于，LaTeX 并不能像插入 .jpg 甚至于矢量图那样便捷地插入 BMP 图片。 BMP文件的数据按照从文件头开始的先后顺序分为四个部分： 位图文件头(bmp file header)： 提供文件的格式、大小等信息 位图信息头(bitmap information)：提供图像数据的尺寸、位平面数、压缩方式、颜色索引等信息 调色板(color palette)：可选，如使用索引来表示图像，调色板就是索引与其对应的颜色的映射表 位图数据(bitmap data)：图像数据区 对于压缩方式，虽然 Bitmap 格式提供简单的压缩功能，但是绝大多数情况下，并没有采用任何压缩手段。 拿最常见的 24BPP RGB （24 比特每像素，红绿蓝三通道） 位图来说，每种颜色需要 8 比特，或者说 1 字节，来存储。在二进制文件中，通常情况下，RGB 按照蓝、绿、红的顺序依次表示图片中的像素点，而 RGBA 则按照蓝、绿、红、透明的顺序（从左下开始，横向逐行向上扫描）。特殊时候，也会出现顺序与上述情况不同的特例，这时色彩顺序会写在 DIB Header 的 Bit Fields 中，以不同色彩通道的 Mask 的形式进行规定。由于 BI_BITFIELDS 也是一种压缩方式，而通常 BMP 不采用任何压缩方式，所以绝大多数时候，我们都是按照前面说的顺序进行排序。 数据按照像素行进行包装，便于读取。但是这并不是全部，因为其中还可能会有补零（zero-padding）。这涉及到计算机的数据结构对齐（data structure alignment）的问题。如果要把BMP讲透彻的话，细节太多，这里就不一一讲解了。想要了解更多，请参考这篇文章 TIF &amp; TIFF TIFF（Tag Image File Format）图像文件是图形图像处理中常用的格式之一，其图像格式很复杂，但由于它对图像信息的存放灵活多变，可以支持很多色彩系统，而且独立于操作系统，因此得到了广泛应用。TIFF文件以.tif为扩展名。其数据格式是一种3级体系结构，Ti内部结构可以分成三个部分，分别是：文件头信息区、标识信息区和图像数据区。其中所有的标签都是以升序排列，这些标签信息是用来处理文件中的图像信息的。在每一个TIFF文件中第一个数据结构称为图像文件头或IFH，它是图像文件体系结构的最高层。这个结构在一个TIFF文件中是惟一的，有固定的位置。它位于文件的开始部分，包含了正确解释TIFF文件的其他部分所需的必要信息。IFD是TIFF文件中第2个数据结构，它是一个名为标记(tag)的用于区分一个或多个可变长度数据块的表，标记中包含了有关于图像的所有信息。IFD提供了一系列的指针(索引)，这些指针告诉我们各种有关的数据字段在文件中的开始位置，并给出每个字段的数据类型及长度。这种方法允许数据字段定位在文件的任何地方，且可以是任意长度，因此文件格式十分灵活。根据IFD所指向的地址，相关的图像信息存储在图像数据区域。 SVG SVG（Scalable Vector Graphics）图像使用XML来定义图形。这种方式定义的图像，在放大缩小，或者改变尺寸以后图片质量都不会有损失。此外，SVG图像可以被很多编辑器编辑，甚至你可以用普通的文本编辑器来编辑SVG图像文件。因此，SVG图像中的文本都是可选的，易搜索的，所以很适合来做地图。一般前端网页DOM都用SVG来添加图片。不过因为SVG实际上是文本描述文件，在表示动图的时候需要频繁的改动，因此SVG动画一般性能较低。","tags":["Computer Vision","Big Data"],"categories":["技术加油站"]},{"title":"OpenCV 入门","path":"/2022/03/24/2022-03-24-openCV-01/","content":"OpenCV 使用 C/C++ 开发，同时也提供了 Python、Java、MATLAB 等其他语言的接口。如果你不了解 C/C++，请阅读《C语言教程》和《C++教程》。 OpenCV 是跨平台的，可以在 Windows、Linux、Mac OS、Android、iOS 等操作系统上运行。 OpenCV 的应用领域非常广泛，包括图像拼接、图像降噪、产品质检、人机交互、人脸识别、动作识别、动作跟踪、无人驾驶等。 IPPICV 加速 如果希望得到更多在英特尔架构上的自动优化，可以购买英特尔的集成性能基元（IPP）库，该库包含了许多算法领域的底层优化程序。在库安装完毕的情况下 OpenCV 在运行的时候会自动调用合适的 IPP 库。 从 OpenCV 3.0 开始，英特尔许可 OpenCV 研发团队和 OpenCV 社区拥有一个免费的 IPP 库的子库（称 IPPICV），该子库默认集成在 OpenCV 中并在运算时发挥效用。如果你使用的是英特尔的处理器，那么 OpenCV 会自动调用 IPPICV。 OpenCV的应用 自从测试版本在 1999 年 1 月发布以来，OpenCV 已经广泛用于许多应用、产品以及科研工作中。这些应用包括在卫星和网络地图上拼接图像，图像扫描校准，医学图像的降噪，目标分析，安保以及工业检测系统，自动驾驶和安全系统，制造感知系统，相机校正，军事应用，无人空中、地面、水下航行器。 它也被运用于声音和音乐的识别，在这些场景中，视觉识别方法被运用于声音的频谱图像。因为计算机的视觉只是一串串数字, OpenCV就是根据统计学,数学和计算机科学在这串数字上的分析建立起来的. OpenCV 的组织关系 OpenCV 是由很多模块组成的，这些模块可以分成很多层： 最底层是基于硬件加速层（HAL）的各种硬件优化。 再上一层是 opencv_contrib 模块所包含的 OpenCV 由其他开发人员所贡献的代码，其包含大多数高层级的函数功能。这就是OpenCV的核心。 接下来是语言绑定和示例应用程序。 处于最上层的是 OpenCV 和操作系统的交互。 OpenCV的模块 下表给出了 OpenCV 包含的具体模块，虽然这些模块会随着时间推移而不断的发展，但模块始终是组成这个库的基本单位，每个函数都是一个模块的一部分。 模块 说明 Core 该模块包含 OpenCV 库的基础结构以及基本操作。 Improc 图像处理模块包含基本的图像转换，包括滤波以及类似的卷积操作。 Highgui 在 OpenCV 3.0中，分割为 imcodecs、videoio 以及 highgui 三部分。这个模块包含可以用来显示图像或者简单的输入的用户交互函数。这可以看作是一个非常轻量级的 Windows UI 工具包。 Video 该模块包含读取和写视频流的函数。 Calib3d 这个模块包括校准单个、双目以及多个相机的算法实现。 Feature2d 这个模块包含用于检测、描述以及匹配特征点的算法。 Objectect 这个模块包含检测特定目标，比如人脸或者行人的算法。也可以训练检测器并用来检测其他物体。 MI 机器学习模块本身是一个非常完备的模块，包含大量的机器学习算法实现并且这些算法都能和 OpenCV 的数据类型自然交互。 Flann Flann 的意思是“快速最邻近库”。这个库包含一些你也许不会直接使用的方法，但是其他模块中的函数会调用它在数据集中进行最邻近搜索。 GPU 在 OpenCV 中被分割为多个 cuda* 模块。GPU 模块主要是函数在 CUDA GPU 上的优化实现，此外，还有一些仅用于 GPU 的功 能。其中一些函数能够返回很好的结果，但是需要足够好的计算资源，如果硬件没有GPU，则不会有什么提升。 Photo 这是一个相当新的模块，包含计算摄影学的一些函数工具。 Stitching 本模块是一个精巧的图像拼接流程实现。这是库中的新功能，但是，就像 Photo 模块一样，这个领域未来预计有很大的增长。 Nonfree 在 OpenCV 3.0 中，被移到 opencv_contrib/xfeatures2d。OpenCV 包含一些受到专利保护的或者受到使用限制的（比如 SIFT 算法）算法。这些算法被隔离到它们自己的模块中，以表明你需要做一些特殊的工作，才可以在商业产品中使用它们。 Contrib 在 OpenCV 3.0 中，融合进了 opencv_contrib。这个模块包含一些新的、还没有被集成进 OpenCV 库的东西。 Legacy 在 OpenCV 3.0 中，被取消。 ocl 在OpenCV 3.0 中，被取消，取而代之的是 T-API。这是一个较新的模块，可以认为它和 GPU 模块相似，它实现了开放并行编程的 Khronos OpenCL 标准。虽然现在模块的特性比 GPU 模块少很多，但 ocl 模块的目标是提供可以运行在任何 GPU 或者是其他可以搭载 Khronos 的并行设备。这与 GPU 模 块形成了鲜明的对比，后者使用 Nividia CUDA 工具包进行开发，因此只能在 Nividia GPU 设备上工作。 OpenCV的下载和安装 OpenCV的下载和安装请参考OpenCV下载和安装（包含所有平台） OpenCV的使用 Find Contour Face Detection","tags":["Computer Vision","OpenCV"],"categories":["技术加油站"]},{"title":"如何在Server上优化博客上图片的加载速度","path":"/2022/03/23/2022-03-23-optimize-img-load/","content":"在使用博客搭建的时候,一般来说我的图片和博客都是开源放在github上的. 但是如果用相对路径来读取图片的话,图片的加载速度收到github流量速度限制,会变得很慢,而且有一些地区也对GitHub加了一道防火墙,这使GitHub Pages加载速度更慢了.目前有两种方式可以缓解图片加载速度. Github Raw 文件读取 使用Github Raw来加载文件通常比相对路径配置的更快些, 可通过以下几种URL来加载raw文件： 123https://raw.githubusercontent.com/user/repo/branch/README.mdhttps://github.com/user/repo/raw/branch/README.mdhttps://github.com/user/repo/blob/branch/README.md?raw=true 对于不支持 Raw 查看的文件，在浏览器中输入或点击上述查看 raw 文件的 URL 时，会对文件进行下载。因此可以通过这个特性，在 GitHub 上直接提供文件的下载功能。支持 Raw 查看的文件，可在浏览器打开 raw 页面，直接 另存为… 保存即可 内容分发网络(Content Delivery Network (CDN)) 关于CDN, 百度百科给出的解释是: CDN是构建在现有网络基础之上的智能虚拟网络，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等功能模块，使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。CDN的关键技术主要有内容存储和分发技术。 CDN的基本原理是广泛采用各种缓存服务器，将这些缓存服务器分布到用户访问相对集中的地区或网络中，在用户访问网站时，利用全局负载技术将用户的访问指向距离最近的工作正常的缓存服务器上，由缓存服务器直接响应用户请求. CDN的基本思路是尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和环节，使内容传输的更快、更稳定。通过在网络各处放置节点服务器所构成的在现有的互联网基础之上的一层智能虚拟网络，CDN系统能够实时地根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。其目的是使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度。根据我的理解,简单的说就是用更近的互联网数据中心(Internet Data Center,IDC)缓存内容,让客户端有更快的速度来访问此项资源. 如何搭建CDN服务 CDN服务: jsDelivr jsDelivr 是一个免费的开源CDN. 这是在中国大陆唯一有 license 的公有 CDN，而且实际使用中的访问速度也是极快的. 它可以引用的资源包括NPM、github、wordpress的所有资源，github可以是任意体积小于50M的仓库。 以github为例，只需要通过符合 JSDelivr 规则的 URL 引用，即可直接使用 Github 中的资源: 123456789101112131415161718192021// load any GitHub release, commit, or branch// note: we recommend using npm for projects that support ithttps://cdn.jsdelivr.net/gh/user/repo@version/file// load jQuery v3.2.1https://cdn.jsdelivr.net/gh/jquery/jquery@3.2.1/dist/jquery.min.js// use a version range instead of a specific versionhttps://cdn.jsdelivr.net/gh/jquery/jquery@3.2/dist/jquery.min.jshttps://cdn.jsdelivr.net/gh/jquery/jquery@3/dist/jquery.min.js// omit the version completely to get the latest one// you should NOT use this in productionhttps://cdn.jsdelivr.net/gh/jquery/jquery/dist/jquery.min.js// add &quot;.min&quot; to any JS/CSS file to get a minified version// if one doesn&#x27;t exist, we&#x27;ll generate it for youhttps://cdn.jsdelivr.net/gh/jquery/jquery@3.2.1/src/core.min.js// add / at the end to get a directory listinghttps://cdn.jsdelivr.net/gh/jquery/jquery/ 详细配置,请参考jsDelivr 这是使用github raw加载和使用CDN来加载图片的对比: GitHub RawjsDelivr(CDN) 其他免费并且不限速的CDN推荐 staticaly 它可以轻松地从GitHub / GitLab / Bitbucket等加载您的项目 没有流量限制或限制。 文件通过超快速全球CDN提供。 在URL（不是分支）中使用特定标记或提交哈希。 根据URL永久缓存文件。 除master分支外，文件在浏览器中缓存1年","tags":["Optimization","CDN"],"categories":["解决方案"]},{"title":"检测模糊图片的几种方法","path":"/2022/03/22/2022-03-22-blur-detection/","content":"为什么要做模糊检测 ? 在机器学习准备数据的时候,有些数据并不尽如人意. 比如在计算机视觉训练时,图片数据集中有一些收集的图片清晰度不够,这会影响到后续的训练和模型的准确度. 准备图片数据的时候一般会根据图片质量选择是否先给图片降噪,然后把较为模糊的图片给去掉. 除此以外还可以用来做自动图像分级, 在视频流做OCR的时候可以过滤掉一些质量不好的图片来节省算力. 本文主要为讨论了模糊图片的检测的两种方法. 方法1: 使用OpenCV和拉普拉斯算子计算 一般来说，如果要检测图片的模糊程度，首先要考虑的方法是计算图像的快速傅里叶变换，然后检查低频和高频的分布：如果图像只有有少量的高频，那么图像就会被认为是模糊的．然而，定义什么算低数量的高频或者什么是高数量的高频是相当困难的。 根据Pertuz 的研究, 仅使用基本的灰度像素强度统计, 评估图像的局部二值模式就可以计算一个单一的浮点值来表示一个给定图像的模糊程度. 一般的,我们使用Tenengrad梯度方法或者Laplacian梯度方法. 这两个不同算法在模糊检测时的结果几近相同,而使用Laplacian算子会减少运算量(Tenengrad梯度方法会计算x,y两个方向的梯度, 而Laplacian梯度方法只用计算Laplacian算子矩阵和灰度图的卷积), 所以我们选择Laplacian算子来检测图片的模糊程度. 下边是使用opencv检测图片的代码 1234567891011import cv2def detect_pic_blur(img_path) -&gt; bool: # read picture img = cv2.imread(img_path) # remove color img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # calculate the Laplacian value of the img fm = cv.Laplacian(gray, cv2.CV_64F).var() print(f&#x27;图片的拉普拉斯值为:&#123;fm&#125;&#x27;) threshold = 120 return False if fm &lt;= threshold else True 使用此方法的话,一般需要对不同的数据集采用不同的threshold. 方法2: 使用OpenCV 和 Fast Fourier Transform (FFT)检测模糊图片 当数据集多的时候,拉普拉斯方法需要大量的手动调整来定义图像是否模糊的“阈值”。如果您可以控制您的照明条件、环境和图像捕获过程，那么它的效果会非常好. 但是如果实验数据收集的时候变量太大(不同拍照设备,不同天气)的话,这样拉普拉斯算子算法就不是一个很好的选择. 这个方法的思路是使用快速傅里叶变换来计算图片的模糊值. 在计算机视觉方面，我们经常将 FFT 视为一种图像处理工具，可以在两个域中表示图像： 傅里叶域 空间域 因此，FFT 用实部和虚部表示图像。通过分析这些值，我们可以执行图像处理例程，例如模糊、边缘检测、阈值处理、纹理分析，甚至是模糊检测。更多关于傅里叶变换的内容请参考FFT and it’s relation to image processing 下边是使用opencv检测图片的代码 12345678910111213141516171819202122def detect_img_blur_fft(img_path, size=60, threshold=10, vis=False): img = cv2.imread(img_path) img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) (height, width) = img_gray.shape # compute center coordinates (cX, cY) = (int(width / 2.0), int(height / 2.0)) # compute FFT fft = np.fft.fft2(img_gray) fft_shift = np.fft.fftshift(fft) # Zero-out the center of our FFT shift (i.e., to remove low frequencies) fft_shift[cY - size:cY + size, cX - size:cX + size] = 0 # Apply the inverse shift to put the DC component back in the top-left fftShift = np.fft.ifftshift(fft_shift) # apply inverse FFT recon = np.fft.ifft2(fftShift) magnitude = 20 * np.log(np.abs(recon)) mean = np.mean(magnitude) print(f&#x27;Mean: &#123;mean&#125;&#x27;) # the image will be considered &quot;blurry&quot; if the mean value of the # magnitudes is less than the threshold value return False if mean &lt;=threshold else True","tags":["Computer Vision","Python","Data Clean"],"categories":["技术加油站"]},{"title":"Hello World","path":"/2022/03/14/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment"},{"title":"Git `.gitignore` 更新后没有作用","path":"/2019/01/03/2019-01-03-git-refresh-gitignore/","content":"Git .ignore 编辑后文件没有变化 如果在文件编辑、修改之后再更新的.gitignore, 那么这个时候Git并不会作用于现在当前文件夹里的文件。因为这个时候Git已经认为这些文件已经是这个Repository的一部分了。这个时候只需要用以下命令来清理缓存然后再重新添加文件就好。 12git rm -rf --cached . # remove cachegit add . # add contents to the index","tags":["Git"],"categories":["解决方案"]},{"title":"关于","path":"/about/index.html","content":"嗨！我是Zhenxiang 👋一个SWE和DS。我的技能列表都有这些: &nbsp;&nbsp; &nbsp;&nbsp; 📫 联系我如果你需要联系我，给我发送邮件是最高效的方法。我会每天查看一次收件箱，所以，除非你的邮件被系统移动到垃圾邮件中（一般我会每周简单检查一下垃圾邮件箱然后直接清空，所以甚至有可能我根本不会看到你的邮件），你将会可以很快收到我的回复。 ℹ️ 本页面使用以下技术制作 &nbsp; 👨🏻‍💻 我的 GitHub GitHub 统计数据 最常用的语言 🗓 近期日程 开源项目无任何盈利目的，所有项目均会在闲暇之余维护，开源项目如有任何问题建议提交 Issue，紧急问题（包括但不限于安全缺陷等问题）请发送私人邮件 即将到来，敬请期待！ 🤩 兴趣爱好 游戏 游戏 推荐指数 游玩时间 Grand Theft Auto V ★★★☆ 2017-2019 TaiTan Fall 2 ★★★★☆ 2021-2022 It Takes Two ★★★★☆ 2020- Human Fall Flat ★★★★☆ 2020- Counter-Strike: Global Offensive ★★★★☆ 2016- Tom Clancy’s Ghost Recon® Wildlands ★★★★ 2017-2018 英雄联盟手游 ★★★★☆ 2021-"},{"path":"/more/index.html","content":"Zhenxiang如果宇宙中真有什么终极的逻辑，那就是我们终有一天会在舰桥上重逢，直到生命终结。"},{"path":"/friends/index.html","content":"海内存知己 天涯若比邻 感谢人生旅途中的每一份真挚的友谊. 如果您想添加友链可以直接邮件联系我. 如何自助添加友链？ 此功能目前还在完善中… 先友后链，在我们有一定了解了之后才可以交换友链，除此之外，您的网站还应满足以下条件： 合法的、非营利性、无商业广告 有实质性原创内容的 HTTPS 站点"},{"title":"认识深度学习","path":"/wiki/deep_learning/index.html","content":"什么是深度学习 ? 深度学习的历史可以追溯到 20 世纪 40 年代。深度学习看似是一个全新的领域，只不过因为在目前流行的前几 年它是相对冷门的，同时也因为它被赋予了许多不同的名称（其中大部分已经不再使用），最近才成为众所周知的 “深度学习’’。这个领域已经更换了很多名称，它反映了不同的研究人员和不同观点的影响。一般来说，目前为止深度学习已经经历了三次发展浪潮：20 世纪 40 年代到 60 年代深度学习的雏形出现在 控制论（cybernetics）中，20 世纪 80 年代 到 90 年代深度学习表现为 联结主义（connectionism），直到 2006 年，才真正以深度学习之名复兴。 现代术语 “深度学习’’ 超越了目前机器学习模型的神经科学观点。它诉诸于学 习多层次组合这一更普遍的原理，这一原理也可以应用于那些并非受神经科学启发的机器学习框架。 深度学习最大的用处是他可以学习非线性函数。最经典的一个机器学习的例子是一个简单的单层神经网络就可以学习到XOR函数。现在，神经科学被视为深度学习研究的一个重要灵感来源，但它已不再是该领域的主要指导。 如今神经科学在深度学习研究中的作用被削弱，主要原因是我们根本没有足够 的关于大脑的信息来作为指导去使用它。要获得对被大脑实际使用算法的深刻理解，我们需要有能力同时监测（至少是）数千相连神经元的活动。我们不能够做到这一点，所以我们甚至连大脑最简单、最深入研究的部分都还远远没有理解。深度学习的另一个最大的成就是其在 强化学习（reinforcement learning）领域的扩展。在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来学习执行任务。DeepMind 表明，基于深度学习的强化学习系统能够学会玩 Atari 视频游戏，并在多种任务中可与人类匹敌 (Mnih et al., 2015)。深度学习也显著改善了机器人强化学习的性能 (Finn et al., 2015)。 总之，深度学习是机器学习的一种方法。在过去几十年的发展中，它大量借鉴了我们关于人脑、统计学和应用数学的知识。近年来，得益于更强大的计算机、更大的数据集和能够训练更深网络的技术，深度学习的普及性和实用性都有了极大的发展。未来几年充满了进一步提高深度学习并将它带到新领域的挑战和机遇。 深度学习的流程 一项机器学习任务时常常有以下的几个重要步骤，首先是数据的预处理，其中重要的步骤包括数据格式的统一、异常数据的消除和必要的数据变换，同时划分训练集、验证集、测试集，常见的方法包括：按比例随机选取，KFold方法（我们可以使用sklearn带的test_train_split函数、kfold来实现）。接下来选择模型，并设定损失函数和优化方法，以及对应的超参数（当然可以使用sklearn这样的机器学习库中模型自带的损失函数和优化器）。最后用模型去拟合训练集数据，并在验证集/测试集上计算模型表现。 深度学习和机器学习在流程上类似，但在代码实现上有较大的差异。首先，由于深度学习所需的样本量很大，一次加载全部数据运行可能会超出内存容量而无法实现；同时还有批（batch）训练等提高模型表现的策略，需要每次训练读取固定数量的样本送入模型中训练，因此深度学习在数据加载上需要有专门的设计。在模型实现上，深度学习和机器学习也有很大差异。由于深度神经网络层数往往较多，同时会有一些用于实现特定功能的层（如卷积层、池化层、批正则化层、LSTM层等），因此深度神经网络往往需要“逐层”搭建，或者预先定义好可以实现特定功能的模块，再把这些模块组装起来。这种“定制化”的模型构建方式能够充分保证模型的灵活性，也对代码实现提出了新的要求。 深度学习中训练和验证过程最大的特点在于读入数据是按批的，每次读入一个批次的数据，放入GPU中训练，然后将损失函数反向传播回网络最前面的层，同时使用优化器调整网络参数。这里会涉及到各个模块配合的问题。训练/验证后还需要根据设定好的指标计算模型表现。 神经网络结构 抽象点讲神经网络是由无数个神经元组成的. 真实点讲的话,就是神经网络是由无数线性函数外套非线性函数组成的. 一般我们叫这一层非线性函数为: 激活函数(Activation Function). 常见的激活函数 常见的激活函数 Sigmoid sigmoid是使用范围最广的一类激活函数，具有指数的形状，它在物理意义上最为接近神经元。sigmoid的输出是（0，1），具有很好的性质，可以被表示做概率或者用于输入的归一化等等。 然而，sigmoid也具有自身的缺陷。第一点，最明显的就是饱和性，从上图也不难看出其两侧导数逐渐趋近于0， 即 。具体来说，在反向传播的过程中，sigmoid的梯度会包含了一个 因子（sigmoid关于输入的导数），因此一旦输入落入两端的饱和区，就会变得接近于0，导致反向传播的梯度也变得非常小，此时网络参数可能甚至得不到更新，难以有效训练，这种现象称为梯度消失。一般来说，sigmoid网络在5层之内就会产生梯度消失现象。 第二点，激活函数的偏移现象。sigmoid函数的输出值均大于0，使得输出不是0的均值，这会导致后一层的神经元将得到上一层非0均值的信号作为输入。 Tanh tanh也是一种非常常见的激活函数，与sigmoid相比，它的输出均值为0，这使得它的收敛速度要比sigmoid快，减少了迭代更新的次数。 然而tanh和sigmoid一样具有饱和性，会造成梯度消失。 Rectified Linear Units (ReLU) ReLU是针对sigmoid和tanh的饱和性二提出的新的激活函数。从上图中可以很容易的看到，当的时候，不存在饱和问题，所以ReLU能够在的时候保持梯度不衰减，从而缓解梯度消失的问题。这让我们可以以有监督的方式训练深度神经网络，而无需依赖无监督的逐层训练。 然而，随着训练的推进，部分输入会落入硬饱和区（即的区域），导致权重无法更新，这种现象称为“神经元死亡”。 而且，与sigmoid类似，ReLU的输出均值也大于0，偏移现象和神经元死亡会共同影响网络的收敛性。 SoftPlus SoftPlus可以作为ReLu的一个不错的替代选择，可以看到与ReLU不同的是，SoftPlus的导数是连续的、非零的、无处不在的，这一特性可以防止出现ReLU中的“神经元死亡”现象。 常见的激活函数 然而，SoftPlus是不对称的，不以0为中心，存在偏移现象；而且，由于其导数常常小于1，也可能会出现梯度消失的问题。 SoftMax Softmax 一般用作输出层, 主要针对分类问题. Softmax 会计算各个种类的概率,而且会normalize 输出使其输出的总和(总概率)为 1. 因此,Softmax 层的节点数必须与输出层的节点数相同。 激活函数:SoftMax Maxout 可以注意到，ReLU 和 Leaky-ReLU 都是它的一个变形。这个激活函数有点大一统的感觉，因为maxout网络能够近似任意连续函数，且当 为0时，退化为ReLU。Maxout能够缓解梯度消失，同时又规避了ReLU神经元死亡的缺点，但增加了参数和计算量。 常见的损失函数 Cross-Entropy Cross-Entropy 本质上也是一种对数似然函数，可用于二分类和多分类任务中。 MDE Hinge 损失函数 hinge损失函数表示如果被分类正确，损失为0，否则损失就为 。SVM就是使用这个损失函数。 ​ 深度学习经典实例：神经网络学习 XOR 算法 XOR 函数（“异或” 逻辑）是两个二进制值 x1 和 x2 的运算。当这些二进制值 中恰好有一个为 1 时，XOR 函数返回值为 1。其余情况下返回值为 0。XOR 函数提供了我们想要学习的目标函数 。我们的模型给出了一个函数 并且我们的学习算法会不断调整参数 θ 来使得 f 尽可能接近 。 在这个简单的例子中， 统计泛化并不重要。首先我们可以把这个问题当作是回归问题，并使用均方误差损失函数。我们选择这 个损失函数是为了尽可能简化本例中用到的数学。在应用领域，对于二进制数据建模时，MSE通常并不是一个合适的损失函数。 假设这是一个线性模型，损失函数我们用MSE，模型的目标函数是. 我们可以使用正规方程来解w和b来试损失函数最小。求解以后得到w=0， b=1/2. 线性模型仅仅是在任意一点都输出 0.5。显然这不是我们想要的结果。 如果我们在此基础上加入一个隐藏层，这个隐藏层的激活函数我们用RELU（rectified linear unit）。这个模型就变成了 最后得到一个函数， 函数图像如下： XOR NN函数 神经网络对这XOR函数的每个样本都给出了正确的结果。在这个例子中，我们简单地指定了解决方案，然后说明它得到的误差为零。在实际情况中，可能会有数十亿的模型参数以及数十亿的训练样本，所以不能像我们这里做的那样进行简单地猜解。与之相对的，基于梯度的优化算法可以找到一些参数使得产生的误差非常小。我们这里给出的 XOR 问题的解处在损失函数的全局最小点，所以梯度下降算法可以收敛到这一点。梯度下降算法还可以找到 XOR 问题一些其他的等价解。梯度下降算法的收敛点取决于参数的初始值。在实践中，梯度下降通常不会找到像我们这里给出的那种干净的、容易理解的、整数值的解。"},{"title":"认识LeetCode","path":"/wiki/leetcode/index.html","content":"本系列不保证是代码是最优解,但是保证一定是可以通过全部测试的解. 本系列旨在提供总结思路,精益求精还要靠自己. 我的理念: Done is better than perfect! 什么是 LeetCode ? LeetCode 是一个刷题网站. 它源自美国硅谷，为全球程序员提供了专业的 IT 技术职业化提升平台，有效帮助程序员实现快速进步和长期成长。像牛客网 一样,你可以在上边尝试解决各种算法问题来磨练你的编程思路与方法. 我能用LeetCode做什么 ? 一般来说,每个题目都会涉及到不同的算法或思路,解决这类问题不仅可以提高我们的编程能力,而且也可以提升我们在解决日常问题时的思维. 这个系列是为了分享与整合LeetCode中题目的思路.","tags":[null]},{"title":"学习Python","path":"/wiki/learn_python/index.html","content":"Python 基础 本系列只针对有一定Python编程基础的人. 如果你对Python一无所知,请先移步到 Runoob 或者 廖雪峰的官方网站-Python来学习关于Python的基础知识. Python小彩蛋1import this 说明：输入上面的代码，在Python的交互式环境中可以看到Tim Peter撰写的“Python之禅”，里面讲述的道理不仅仅适用于Python，也适用于其他编程语言。 Why Python 很多互联网和移动互联网企业对开发效率的要求高于对执行效率的要求。 很多时候，你的问题只需一行Python代码就能解决。 1234567891011121314# 一行代码实现求阶乘函数fac = lambda x: __import__(&#x27;functools&#x27;).reduce(int.__mul__, range(1, x + 1), 1)# 一行代码实现求最大公约数函数gcd = lambda x, y: y % x and gcd(y % x, x) or x# 一行代码实现判断素数的函数is_prime = lambda x: x &gt; 1 and not [f for f in range(2, int(x ** 0.5) + 1) if x % f == 0]# 一行代码实现快速排序quick_sort = lambda items: len(items) and quick_sort([x for x in items[1:] if x &lt; items[0]]) + [items[0]] + quick_sort([x for x in items[1:] if x &gt; items[0]]) or items# 生成FizzBuzz列表[&#x27;Fizz&#x27;[x % 3 * 4:] + &#x27;Buzz&#x27;[x % 5 * 4:] or x for x in range(1, 101)] 设计模式从未如此简单 Python是动态类型语言，大量的设计模式在Python中被简化或弱化. 思考：如何优化下面的代码。 1234def fib(num): if num in (1, 2): return 1 return fib(num - 1) + fib(num - 2) 代理模式在Python中可以通过内置的或自定义的装饰器来实现。 123456789101112from functools import lru_cache@lru_cache()def fib(num): if num in (1, 2): return 1 return fib(num - 1) + fib(num - 2)for n in range(1, 121): print(f&#x27;&#123;n&#125;: &#123;fib(n)&#125;&#x27;) 说明：通过Python标准库functools模块的lru_cache装饰器为fib函数加上缓存代理，缓存函数执行的中间结果，优化代码的性能。 写出Python代码的正确姿势 用Python就要写出Pythonic的代码. 交换变量 跨界开发者的代码: 123temp = aa = bb = temp 或者 123a = a ^ bb = a ^ ba = a ^ b Pythonic的代码: 1a, b = b, a 用序列组装字符串 跨界开发者的代码： 1234chars = [&#x27;j&#x27;, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;k&#x27;, &#x27;f&#x27;, &#x27;r&#x27;, &#x27;u&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]name = &#x27;&#x27;for char in chars: name += char Pythonic的代码： 12chars = [&#x27;j&#x27;, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;k&#x27;, &#x27;f&#x27;, &#x27;r&#x27;, &#x27;u&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]name = &#x27;&#x27;.join(chars) 创建列表 跨界开发者的代码： 12345data = [7, 20, 3, 15, 11]result = []for i in data: if i &gt; 10: result.append(i * 3) Pythonic的代码： 12data = [7, 20, 3, 15, 11]result = [num * 3 for num in data if num &gt; 10] 遍历列表 跨界开发者的代码: 12345fruits = [&#x27;orange&#x27;, &#x27;grape&#x27;, &#x27;pitaya&#x27;, &#x27;blueberry&#x27;]index = 0for fruit in fruits: print(index, &#x27;:&#x27;, fruit) index += 1 Pythonic的代码: 123fruits = [&#x27;orange&#x27;, &#x27;grape&#x27;, &#x27;pitaya&#x27;, &#x27;blueberry&#x27;]for index, fruit in enumerate(fruits): print(index, &#x27;:&#x27;, fruit) 确保代码健壮性 跨界开发者的代码： 1234567data = &#123;&#x27;x&#x27;: &#x27;5&#x27;&#125;if &#x27;x&#x27; in data and isinstance(data[&#x27;x&#x27;], (str, int, float)) \\ and data[&#x27;x&#x27;].isdigit(): value = int(data[&#x27;x&#x27;]) print(value)else: value = None Pythonic的代码： 123456data = &#123;&#x27;x&#x27;: &#x27;5&#x27;&#125;try: value = int(data[&#x27;x&#x27;]) print(value)except (KeyError, TypeError, ValueError): value = None Python 代码规范 使用Lint工具检查你的代码规范 阅读下面的代码，看看你能看出哪些地方是有毛病的或者说不符合Python的编程规范的。 1234567891011121314151617181920212223242526272829303132333435363738from enum import *@uniqueclass Suite (Enum): SPADE, HEART, CLUB, DIAMOND = range(4)class Card(object): def __init__(self,suite,face ): self.suite = suite self.face = face def __repr__(self): suites=&#x27;♠♥♣♦&#x27; faces=[&#x27;&#x27;,&#x27;A&#x27;,&#x27;2&#x27;,&#x27;3&#x27;,&#x27;4&#x27;,&#x27;5&#x27;,&#x27;6&#x27;,&#x27;7&#x27;,&#x27;8&#x27;,&#x27;9&#x27;,&#x27;10&#x27;,&#x27;J&#x27;,&#x27;Q&#x27;,&#x27;K&#x27;] return f&#x27;&#123;suites[self.suite.value]&#125;&#123;faces[self.face]&#125;&#x27;import randomclass Poker(object): def __init__(self): self.cards =[Card(suite, face) for suite in Suite for face in range(1, 14)] self.current=0 def shuffle (self): self.current=0 random.shuffle(self.cards) def deal (self): card = self.cards[self.current] self.current+=1 return card def has_next (self): if self.current&lt;len(self.cards): return True return Falsep = Poker()p.shuffle()print(p.cards) PyLint的安装和使用 Pylint是Python代码分析工具，它分析Python代码中的错误，查找不符合代码风格标准（默认使用的代码风格是 PEP 8）和有潜在问题的代码。 12pip install pylintpylint [options] module_or_package Pylint输出格式如下所示。 模块名:行号:列号: 消息类型 消息 消息类型有以下几种： C - 惯例：违反了Python编程惯例（PEP 8）的代码。 R - 重构：写得比较糟糕需要重构的代码。 W - 警告：代码中存在的不影响代码运行的问题。 E - 错误：代码中存在的影响代码运行的错误。 F - 致命错误：导致Pylint无法继续运行的错误。 Pylint命令的常用参数： --disable=&lt;msg ids&gt;或-d &lt;msg ids&gt;：禁用指定类型的消息。 --errors-only或-E：只显示错误。 --rcfile=&lt;file&gt;：指定配置文件。 --list-msgs：列出Pylint的消息清单。 --generate-rcfile：生成配置文件的样例。 --reports=&lt;y_or_n&gt;或-r &lt;y_or_n&gt;：是否生成检查报告。","tags":[null]},{"title":"反向传播算法 (Back Propagation)","path":"/wiki/deep_learning/backpropagation/backpropagation.html","content":"反向传播是深度学习的根基.反向传播算法可以快速的计算出各个结构层中的梯度,从而优化各个层级中的参数. 矩阵运算 在此之前你需要知道矩阵运算. 矩阵运算可以参考这个网站，讲解的很详细。 正向传播 (forward propagation) 设每一层的 node 的值z = wx + b. 应用激活函数以后的值a. 模型的输出y^，真实值y. 正向传播（Forward Propagation）前向传播就是从input，经过一层层的layer，不断计算每一层的z和a，最后得到输出y^的过程，计算出了y^，就可以根据它和真实值y的差别来计算损失（loss）。 反向传播 (back propagation) 反向传播（Backward Propagation）反向传播就是根据损失函数L(y^,y)来反方向地计算每一层的z、a、w、b的偏导数（梯度），从而更新参数。 微积分中的链式法则（为了不与概率中的链式法则相混淆）用于计算复合函数 的导数。反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。 反向传播说白了根据根据J的公式对w和b求偏导，也就是求梯度。因为我们需要用梯度下降法来对参数进行更新，而更新就需要梯度。 但是，根据求偏导的链式法则我们知道，第l层的参数的梯度，需要通过l+1层的梯度来求得，因此我们求导的过程是“反向”的，这也就是为什么叫“反向传播”。 计算实例 使用代码来计算matrix."},{"title":"计算机视觉介绍","path":"/wiki/deep_learning/cv_intro/index.html","content":"计算机视觉的任务 计算机视觉的任务主要有4种: Image Classification: 最简单的任务,根据识别的图片来进行分类. Object Detection: 最常用的任务之一,可以预测物体的位置和种类. Semantic Segmentation: Instance Segmentaion 细分下去的话,还有人体关键点检测,光学字符识别,目标跟踪,行为识别,全景感知等. OpenMMLab提供了很多开源的模型可以直接来使用学习. 计算机视觉的发展 传统机器学习和模式识别 1964年, 计算机视觉刚开始起步的时候,人们只能计算图片的边缘检测. 这也展示了计算机视觉的一个分支. 这个分支影响了计算机视觉领域很久,直到现在这种依靠纯算法的分支还是生产环境/科研中不可或缺的. 比如自动驾驶和机器人SLAM都是大量依靠传统模型计算来获取环境信息的. AI ImageNet(2006) 斯坦福大学的李飞飞教授在2006年启动了ImageNet项目. 这是人类当时最大的可以用来训练模型的数据库.ImageNet包含了20,000个类,共计约1500万张图片. ~2010: 初有成效 特征还是人工提取的. 比如ImageNet结构. 当时已经有很好的识别效果了. 2012~:深度学习时代 AlexNet 突破了传统视觉系统的性能问题,使损失函数的损失大大降低,甚至很多时候的表现高于人类自身. Fast R-CNN使目标检测进入深度学习时代. 开源是AI领域发展的引擎 自从2007年Theano开源深度学习框架一来,越来越多的框架被分享出来.比如现在我们熟知的 TensorFlow和Pytorch. 其中值得一提的是 OpenMMLab. 它是在Pytorch框架基础上做出的开源的AI工具库,近几年在各大AI比赛中屠榜. 其开源项目star数超过了60,000, 开发人员全球近10,000人. 其模型架构支持超过1,200篇论文的发表.有兴趣的可以去研究一下. 计算机机视觉常用的公开的数据集 下边是一些常用的综合的数据集的汇总, 如果有其他需求,也可以参考[计算机视觉常用数据集总结：包括MS COCO、ImageNet、VOC、人脸识别、行人检测等 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/129736067#:~:text=本篇博文主要对目前公开的计算机视觉常用数据集进行总结。 PASCAL VOC PASCAL VOC是一个图像集，由Mark Everingham (University ofLeeds)、Luc van Gool (ETHZ, Zurich)等人创立，有1.7W+张图片，分为20类。PASCALVOC竞赛也是计算机视觉竞赛的鼻祖，从2005年到2012年一共举办了8届，包含了物体分类（Classification）、目标检测（Detection）、图像分割（Segmentation）、Person Layout等任务，后来逐渐被ILSVRC竞赛替代. ImageNet ImageNet是一个图像集，由斯坦福大学李飞飞创立，有1400W+张样例图片，分为27大类和2W+小类，只能用于非商业研究和教学使用。与ImageNet图像集相应的是著名的ILSVRC竞赛，各种新机器学习算法脱颖而出（AlexNet、ZFNet、GoogleNet、ResNet、…），图像识别率得以显著提高. COCO MS COCO的全称是Microsoft Common Objects in Context，起源于微软于2014年出资标注的Microsoft COCO数据集，与ImageNet竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。 COCO数据集是一个大型的、丰富的物体检测，分割和字幕数据集。这个数据集以scene understanding为目标，主要从复杂的日常场景中截取，图像中的目标通过精确的segmentation进行位置的标定。图像包括91类目标，328,000影像和2,500,000个label。目前为止有语义分割的最大数据集，提供的类别有80 类，有超过33 万张图片，其中20 万张有标注，整个数据集中个体的数目超过150 万个。"},{"title":"生成对抗网络基础","path":"/wiki/deep_learning/generative_adversarial_network/intro.html","content":"生成式对抗网络（GAN, Generative Adversarial Networks ）是一种深度学习模型，是近年来复杂分布上无监督学习最具前景的方法之一。 GAN是什么 生成式对抗网络（GAN, Generative Adversarial Networks ）是一种深度学习模型，是近年来复杂分布上无监督学习最具前景的方法之一。模型通过框架中（至少）两个模块：生成模型（Generative Model）和判别模型（Discriminative Model）的互相博弈学习产生相当好的输出。原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。 生成模型和判别模型机器学习的模型可大体分为两类，生成模型（Generative Model）和判别模型（Discriminative Model）。判别模型需要输入变量 ，通过某种模型来预测 。生成模型是给定某种隐含信息，来随机产生观测数据。举个简单的例子： 生成模型：给一系列猫的图片，生成一张新的猫咪（不在数据集里） 判别模型：给定一张图，判断这张图里的动物是猫还是狗 对于判别模型，损失函数是容易定义的，因为输出的目标相对简单。但对于生成模型，损失函数的定义就不是那么容易。我们对于生成结果的期望，往往是一个暧昧不清，难以数学公理化定义的范式。所以不妨把生成模型的回馈部分，交给判别模型处理。这就是Goodfellow他将机器学习中的两大类模型，Generative和Discrimitive给紧密地联合在了一起 GAN的应用 图像生成 GAN最常使用的地方就是图像生成，如超分辨率任务，语义分割等等。 数据增强 用GAN生成的图像来做数据增强，如图。主要解决的问题是： 对于小数据集，数据量不足， 如果能生成一些就好了。 如果GAN生成了图片，怎么给这些数据label呢？因为他们相比原始数据也不属于预定义的类别。 在这个论文中，作者做了一些尝试。实验想法也特别简单，先用原始数据（即使只有2000张图）训练一个GAN，然后生成图片，加入到训练集中。 总结一下就是： GAN 生成数据是可以用在实际的图像问题上的（不仅仅是像MNIST这种“toy dataset”上通过实验）。GAN在两个行人重识别数据集和一个细粒度识别鸟识别数据集上都有提升。 GAN 数据有三种给pseudo label的方式， 假设我们做五分类： 把生成的数据都当成新的一类, 六分类，那么生成图像的 label 就可以是 （0, 0, 0, 0, 0, 1） 这样给。 按照置信度最高的动态去分配，那个概率高就给谁 比如第三类概率高（0, 0, 1, 0, 0） 既然所有类都不是，那么可以参考inceptionv3，搞label smooth，每一类置信度相同（0.2, 0.2, 0.2, 0.2, 0.2） 注：作者16年12月写的代码，当时GAN效果没有那么好，用这个效果好也是可能的， 因为生成样本都不是很“真”，所以起到了正则作用。 GAN 的设计初衷 一句话来概括 GAN 的设计动机就是——自动化。深度学习最特别最厉害的地方就是能够自己学习特征提取。GAN 可以自我评估结果然后继续训练,这是一种效率非常高，且成本很低的方式.这也是为什么它是一种无监督学习. GAN 的原理 生成对抗网络（GAN）由2个重要的部分构成： 生成器(Generator)：通过机器生成数据（大部分情况下是图像），目的是“骗过”判别器 判别器(Discriminator)：判断这张图像是真实的还是机器生成的，目的是找出生成器做的“假数据” 第一阶段: 固定“判别器D”，训练“生成器G” 我们使用一个还 OK 判别器，让一个“生成器G”不断生成“假数据”，然后给这个“判别器D”去判断。 一开始，“生成器G”还很弱，所以很容易被揪出来。 但是随着不断的训练，“生成器G”技能不断提升，最终骗过了“判别器D”。 到了这个时候，“判别器D”基本属于瞎猜的状态，判断是否为假数据的概率为50%。 第二阶段：固定“生成器G”，训练“判别器D” 当通过了第一阶段，继续训练“生成器G”就没有意义了。这个时候我们固定“生成器G”，然后开始训练“判别器D”。 “判别器D”通过不断训练，提高了自己的鉴别能力，最终他可以准确的判断出所有的假图片。 到了这个时候，“生成器G”已经无法骗过“判别器D”。 循环阶段一和阶段二 通过不断的循环，“生成器G”和“判别器D”的能力都越来越强。 最终我们得到了一个效果非常好的“生成器G”，我们就可以用它来生成我们想要的图片了。 如果对 GAN 的详细技术原理感兴趣，可以看看下面2篇文章： [easyai]《生成性对抗网络（GAN）初学者指南 – 附代码》 [easyai]《长文解释生成对抗网络GAN的详细原理（20分钟阅读）》 GAN 的优缺点 left 优点能更好建模数据分布(图像更锐利、清晰).理论上，GANs 能训练任何一种生成器网络。其他的框架需要生成器网络有一些特定的函数形式，比如输出层是高斯的。无需利用马尔科夫链反复采样，无需在学习过程中进行推断，没有复杂的变分下界，避开近似计算棘手的概率的难题。 right 缺点难训练，不稳定。生成器和判别器之间需要很好的同步，但是在实际训练中很容易D收敛，G发散。D/G 的训练需要精心的设计。模式缺失（Mode Collapse）问题。GANs的学习过程可能出现模式缺失，生成器开始退化，总是生成同样的样本点，无法继续学习。"},{"title":"LeetCode 200. 岛屿数量","path":"/wiki/leetcode/graph/leetcode_200_number_of_islands.html","content":"这道题可以用暴力解法: 直接全部按照顺序遍历,然后打上标记,然后遍历的时候根据标记来计算是否是同一个岛屿,然后再更改标记. 这个解法逻辑太复杂, 不推荐. 可以想想成一个雷达在扫描这张地图,然后雷达扫描的线经过的地方可以根据之前的’记录’来看是否是岛屿. 这里最简单的是用 BFS, 搜过的点都标记为 0 或者visited, 然后 BFS 的时候只搜同一个岛屿,而且把同一个岛屿击沉.这样的话只需要两个 for 循环, 一个递归就可以解决. 代码如下: 1234567891011121314151617181920212223class Solution: def numIslands(self, grid: List[List[str]]) -&gt; int: count = 0 def dfs(grid, row, column): grid[row][column] = &#x27;0&#x27; # top if row - 1 &gt;= 0 and grid[row-1][column] == &#x27;1&#x27;: dfs(grid, row-1, column) # left if column - 1 &gt;= 0 and grid[row][column-1] == &#x27;1&#x27;: dfs(grid, row, column-1) # bottom if row + 1 &lt; len(grid) and grid[row+1][column] == &#x27;1&#x27;: dfs(grid, row+1, column) # right if column + 1 &lt; len(grid[0]) and grid[row][column+1] == &#x27;1&#x27;: dfs(grid, row, column+1) for i, row in enumerate(grid): for j, num in enumerate(row): if num == &#x27;1&#x27;: count += 1 dfs(grid, i, j) return count","tags":[null]},{"title":"Pandas中基本的数据结构和用法","path":"/wiki/deep_learning/pandas/pandas_data_structure_and_usage.html","content":"Pandas 是使用 Python 在机器学习中不可绕过的必续用到的库. 熟悉和熟练使用这个库可以有效的帮助模型训练和数据清洗. Pandas中的数据结构 Pandas 常用的数据结构有两个,一个是 Series,一个是 DataFrame. 一般的,默认情况下其中的数据是对齐的. 标签和数据之间的联系不会被打破，除非手动更改。 DataFrame 可以看成一个 Excel 表格中的一个 Tab 页面,同时也可以被认为是有限个的Series组合而来的. Note本文的的源码都可以在此 Github Repo 中找到. Series Series 可以看成 Excel 表格中的一行或者一列,这一行/列同时还拥有 Excel 中行/列的 index 信息.当然这个 index 的信息在 Series 中是可以随意更改的. 123# import library. Pandas offten used with Numpy.import numpy as npimport pandas as pd Series 中的基础属性 name dtype 创建新的 Series. 创建 series 的语法是: s = pd.Series(data, index=index). data 可以是 python dict, ndarray,也可以是一个向量值. Index 可以不设置,方法会用默认的数字排序来作为 index. 从ndarray中创建 12345678910111213141516171819202122In [3]: s = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])In [4]: sOut[4]: a 0.469112b -0.282863c -1.509059d -1.135632e 1.212112dtype: float64In [5]: s.indexOut[5]: Index(['a', 'b', 'c', 'd', 'e'], dtype='object')In [6]: pd.Series(np.random.randn(5))Out[6]: 0 -0.1732151 0.1192092 -1.0442363 -0.8618494 -2.104569dtype: float64 从python dict中创建 123456789101112131415161718192021222324In [7]: d = {\"b\": 1, \"a\": 0, \"c\": 2}In [8]: pd.Series(d)Out[8]: b 1a 0c 2dtype: int64In [9]: d = {\"a\": 0.0, \"b\": 1.0, \"c\": 2.0}In [10]: pd.Series(d)Out[10]: a 0.0b 1.0c 2.0dtype: float64In [11]: pd.Series(d, index=[\"b\", \"c\", \"d\", \"a\"])Out[11]: b 1.0c 2.0d NaNa 0.0dtype: float64 NoteNaN(Not a Number) 用来表示数据丢失 从向量值中创建 12345678In [12]: pd.Series(5.0, index=[\"a\", \"b\", \"c\", \"d\", \"e\"])Out[12]: a 5.0b 5.0c 5.0d 5.0e 5.0dtype: float64 Series 的操作 选取与切片 像 ndarray那样操作 Series 的操作非常像ndarray而且很多Numpy functions可以直接用在 Series 上. 比如:取值和 index 切割. 12345678910111213141516171819202122232425262728293031In [13]: s[0]Out[13]: 0.4691122999071863In [14]: s[:3]Out[14]: a 0.469112b -0.282863c -1.509059dtype: float64In [15]: s[s &gt; s.median()]Out[15]: a 0.469112e 1.212112dtype: float64In [16]: s[[4, 3, 1]]Out[16]: e 1.212112d -1.135632b -0.282863dtype: float64In [17]: np.exp(s)Out[17]: a 1.598575b 0.753623c 0.221118d 0.321219e 3.360575dtype: float64 此外 Series像Numpy array一样拥有一个dtype的值. 12In [18]: s.dtypeOut[18]: dtype('float64') 像python dict 那样操作 12345678910111213141516171819202122232425In [21]: s[\"a\"]Out[21]: 0.4691122999071863In [22]: s[\"e\"] = 12.0In [23]: sOut[23]: a 0.469112b -0.282863c -1.509059d -1.135632e 12.000000dtype: float64In [24]: \"e\" in sOut[24]: TrueIn [25]: \"f\" in sOut[25]: False# 如果方括号中的 key 不存在,那么就会返回一个KeyError. 因此取值的时候推荐使用 get()# 使用 get() 方法的时候,及时 key 不存在,也不会报错而会返回 None 或者一个自定义的值.In [27]: s.get(\"f\")In [28]: s.get(\"f\", np.nan)Out[28]: nan Series 到其他类型的转化 如果需要 array,可以使用 Series.array来返回一个 array. 这时就可以做一些不需要 index 的操作. 123456789In [19]: s.arrayOut[19]: &lt;PandasArray&gt;[ 0.4691122999071863, -0.2828633443286633, -1.5090585031735124, -1.1356323710171934, 1.2121120250208506]Length: 5, dtype: float64# 也可以转成 numpy ndarrayIn [20]: s.to_numpy()Out[20]: array([ 0.4691, -0.2829, -1.5091, -1.1356, 1.2121]) 向量操作和标签排列 1234567891011121314151617181920212223242526272829303132333435In [29]: s + sOut[29]: a 0.938225b -0.565727c -3.018117d -2.271265e 24.000000dtype: float64In [30]: s * 2Out[30]: a 0.938225b -0.565727c -3.018117d -2.271265e 24.000000dtype: float64In [31]: np.exp(s)Out[31]: a 1.598575b 0.753623c 0.221118d 0.321219e 162754.791419dtype: float64In [32]: s[1:] + s[:-1]Out[32]: a NaNb -0.565727c -3.018117d -2.271265e NaNdtype: float64 前文提到 Series 的 name 属性,一般 name 会被自动赋值.如果想要使用不同的 name 值,可以使用rename方法. rename 方法每次都会返回一个新的 Series 而不会对之前的 Series 进行任何操作. 1234567891011121314151617In [33]: s = pd.Series(np.random.randn(5), name=\"something\")In [34]: sOut[34]: 0 -0.4949291 1.0718042 0.7215553 -0.7067714 -1.039575Name: something, dtype: float64In [35]: s.nameOut[35]: 'something'In [36]: s2 = s.rename(\"different\")In [37]: s2.nameOut[37]: 'different' DataFrame DataFrame是一个 2D 的带有 label 的数据结构.Dataframe 的每一列都可以是不同的数据类型. 创建新的 DataFrame 创建DataFrame的时候可以直接传递 index(行 label) 和 columns(列 label) 的参数. DataFrame 可以从很多种数据类型种创建: Dict of 1D ndarrays, lists, dicts, or Series 2-D numpy.ndarray Structured or record ndarray A Series Another DataFrame 0. 初始化带有初始值的 DataFrame 1234567891011# initial DataFrame with initial values# !! must contains index and columns !!&gt;&gt;&gt; initial_val = 0&gt;&gt;&gt; index=[\"a\", \"b\", \"c\"]&gt;&gt;&gt; columns=[1,2]&gt;&gt;&gt; df0 = pd.DataFrame(initial_val, index=index, columns=columns)&gt;&gt;&gt; df0 1 2a 0 0b 0 0c 0 0 1. 从Dict of Series or Dict of dicts 中创建 12345678910111213141516171819202122232425262728293031323334In [38]: d = { ....: \"one\": pd.Series([1.0, 2.0, 3.0], index=[\"a\", \"b\", \"c\"]), ....: \"two\": pd.Series([1.0, 2.0, 3.0, 4.0], index=[\"a\", \"b\", \"c\", \"d\"]), ....: } ....: In [39]: df = pd.DataFrame(d)In [40]: dfOut[40]: one twoa 1.0 1.0b 2.0 2.0c 3.0 3.0d NaN 4.0In [41]: pd.DataFrame(d, index=[\"d\", \"b\", \"a\"])Out[41]: one twod NaN 4.0b 2.0 2.0a 1.0 1.0In [42]: pd.DataFrame(d, index=[\"d\", \"b\", \"a\"], columns=[\"two\", \"three\"])Out[42]: two threed 4.0 NaNb 2.0 NaNa 1.0 NaNIn [43]: df.indexOut[43]: Index(['a', 'b', 'c', 'd'], dtype='object')In [44]: df.columnsOut[44]: Index(['one', 'two'], dtype='object') Note当一组特定的列与数据的dict一起被传递时，传递的列会覆盖dict中的键。 2. 从Dict of ndarrays/list 中创建 1234567891011121314151617In [45]: d = {\"one\": [1.0, 2.0, 3.0, 4.0], \"two\": [4.0, 3.0, 2.0, 1.0]}In [46]: pd.DataFrame(d)Out[46]: one two0 1.0 4.01 2.0 3.02 3.0 2.03 4.0 1.0In [47]: pd.DataFrame(d, index=[\"a\", \"b\", \"c\", \"d\"])Out[47]: one twoa 1.0 4.0b 2.0 3.0c 3.0 2.0d 4.0 1.0 3. 从 structured 或者 record array 中创建 这种情况和从 dict of arrays 中创建是一样的. 123456789101112131415161718192021In [48]: data = np.zeros((2,), dtype=[(\"A\", \"i4\"), (\"B\", \"f4\"), (\"C\", \"a10\")])In [49]: data[:] = [(1, 2.0, \"Hello\"), (2, 3.0, \"World\")]In [50]: pd.DataFrame(data)Out[50]: A B C0 1 2.0 b'Hello'1 2 3.0 b'World'In [51]: pd.DataFrame(data, index=[\"first\", \"second\"])Out[51]: A B Cfirst 1 2.0 b'Hello'second 2 3.0 b'World'In [52]: pd.DataFrame(data, columns=[\"C\", \"A\", \"B\"])Out[52]: C A B0 b'Hello' 1 2.01 b'World' 2 3.0 Note虽然DataFrame看起来就像是二维的 Numpy ndarray. 不过它并不完全像二维的NumPy ndarray那样而且一直没有想着去代替 ndarray。 4. 从 list of dicts 中创建 12345678910111213141516171819In [53]: data2 = [{\"a\": 1, \"b\": 2}, {\"a\": 5, \"b\": 10, \"c\": 20}]In [54]: pd.DataFrame(data2)Out[54]: a b c0 1 2 NaN1 5 10 20.0In [55]: pd.DataFrame(data2, index=[\"first\", \"second\"])Out[55]: a b cfirst 1 2 NaNsecond 5 10 20.0In [56]: pd.DataFrame(data2, columns=[\"a\", \"b\"])Out[56]: a b0 1 21 5 10 5. 从 dict of tuples 中创建 12345678910111213141516In [57]: pd.DataFrame( ....: { ....: (\"a\", \"b\"): {(\"A\", \"B\"): 1, (\"A\", \"C\"): 2}, ....: (\"a\", \"a\"): {(\"A\", \"C\"): 3, (\"A\", \"B\"): 4}, ....: (\"a\", \"c\"): {(\"A\", \"B\"): 5, (\"A\", \"C\"): 6}, ....: (\"b\", \"a\"): {(\"A\", \"C\"): 7, (\"A\", \"B\"): 8}, ....: (\"b\", \"b\"): {(\"A\", \"D\"): 9, (\"A\", \"B\"): 10}, ....: } ....: ) ....: Out[57]: a b b a c a bA B 1.0 4.0 5.0 8.0 10.0 C 2.0 3.0 6.0 7.0 NaN D NaN NaN NaN NaN 9.0 6. 从 Series 直接创建 12345678In [58]: ser = pd.Series(range(3), index=list(\"abc\"), name=\"ser\")In [59]: pd.DataFrame(ser)Out[59]: sera 0b 1c 2 7. 从 list of namedtuples 中创建 12345678910111213141516171819In [60]: from collections import namedtupleIn [61]: Point = namedtuple(\"Point\", \"x y\")In [62]: pd.DataFrame([Point(0, 0), Point(0, 3), (2, 3)])Out[62]: x y0 0 01 0 32 2 3In [63]: Point3D = namedtuple(\"Point3D\", \"x y z\")In [64]: pd.DataFrame([Point3D(0, 0, 0), Point3D(0, 3, 5), Point(2, 3)])Out[64]: x y z0 0 0 0.01 0 3 5.02 2 3 NaN 8. 从 list of dataclasses 创建 12345678910In [65]: from dataclasses import make_dataclassIn [66]: Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])In [67]: pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])Out[67]: x y0 0 01 0 32 2 3 9. 使用构造器 from_dict() 创建 DataFrame.from_dict 这个构造器接收一个dict的dict或一个数组类序列的dict，并返回一个DataFrame。 1234567891011121314151617In [68]: pd.DataFrame.from_dict(dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]))Out[68]: A B0 1 41 2 52 3 6# 如果你使用 orient='index', key 就会成为行的标签In [69]: pd.DataFrame.from_dict( ....: dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]), ....: orient=\"index\", ....: columns=[\"one\", \"two\", \"three\"], ....: ) ....: Out[69]: one two threeA 1 2 3B 4 5 6 10. 使用构造器 from_records() 创建 DataFrame.from_records()接收一个元列表或一个具有结构化dtype的ndarray。它的工作方式与普通的DataFrame构造函数类似，只是产生的DataFrame索引可能是结构化dtype的一个特定字段。 1234567891011In [70]: dataOut[70]: array([(1, 2., b'Hello'), (2, 3., b'World')], dtype=[('A', '&lt;i4'), ('B', '&lt;f4'), ('C', 'S10')])In [71]: pd.DataFrame.from_records(data, index=\"C\")Out[71]: A BC b'Hello' 1 2.0b'World' 2 3.0 行和列的选择,添加和删除 Operation Syntax Result Select column df[col] Series Select row by label df.loc[label] Series Select row by integer location df.iloc[index] Series Slice rows df[4:8] DataFrame Select rows by boolean vector df[bool_vec] DataFrame 当成 dict 直接使用 你可以直接把DataFrame当成一个类似索引的Series对象的dict。获取、设置和删除列的语法与类似的dict操作相同。 12345678910111213141516171819In [72]: df[\"one\"]Out[72]: a 1.0b 2.0c 3.0d NaNName: one, dtype: float64In [73]: df[\"three\"] = df[\"one\"] * df[\"two\"]In [74]: df[\"flag\"] = df[\"one\"] &gt; 2In [75]: dfOut[75]: one two three flaga 1.0 1.0 1.0 Falseb 2.0 2.0 4.0 Falsec 3.0 3.0 9.0 Trued NaN 4.0 NaN False 列可以被像字典那样 pop 或 delete 1234567891011In [76]: del df[\"two\"]In [77]: three = df.pop(\"three\")In [78]: dfOut[78]: one flaga 1.0 Falseb 2.0 Falsec 3.0 Trued NaN False 使用其他方法来操作 可以使用insert()或者assign()方法来插入值. 插入值的时候可以直接像字典一样赋值也可以用insert()方法. 1234567891011121314151617181920In [79]: df[\"foo\"] = \"bar\"In [80]: dfOut[80]: one flag fooa 1.0 False barb 2.0 False barc 3.0 True bard NaN False bar# insert 方法In [83]: df.insert(1, \"bar\", df[\"one\"])In [84]: dfOut[84]: one bar flag foo one_trunca 1.0 1.0 False bar 1.0b 2.0 2.0 False bar 2.0c 3.0 3.0 True bar NaNd NaN NaN False bar NaN 当插入一个Series到不同 index 的 DataFrame中的时候,按照 DataFrame 的 index 来更新数据. 123456789In [81]: df[\"one_trunc\"] = df[\"one\"][:2]In [82]: dfOut[82]: one flag foo one_trunca 1.0 False bar 1.0b 2.0 False bar 2.0c 3.0 True bar NaNd NaN False bar NaN 使用assign()方法: 12345678910111213141516171819In [85]: iris = pd.read_csv(\"data/iris.data\")In [86]: iris.head()Out[86]: SepalLength SepalWidth PetalLength PetalWidth Name0 5.1 3.5 1.4 0.2 Iris-setosa1 4.9 3.0 1.4 0.2 Iris-setosa2 4.7 3.2 1.3 0.2 Iris-setosa3 4.6 3.1 1.5 0.2 Iris-setosa4 5.0 3.6 1.4 0.2 Iris-setosaIn [87]: iris.assign(sepal_ratio=iris[\"SepalWidth\"] / iris[\"SepalLength\"]).head()Out[87]: SepalLength SepalWidth PetalLength PetalWidth Name sepal_ratio0 5.1 3.5 1.4 0.2 Iris-setosa 0.6862751 4.9 3.0 1.4 0.2 Iris-setosa 0.6122452 4.7 3.2 1.3 0.2 Iris-setosa 0.6808513 4.6 3.1 1.5 0.2 Iris-setosa 0.6739134 5.0 3.6 1.4 0.2 Iris-setosa 0.720000 assign 的时候也可以使用 lambda function 123456789101112131415161718# assign方法总是返回一个新的 DataFrame 而不会对之前的数据做任何操作.In [88]: iris.assign(sepal_ratio=lambda x: (x[\"SepalWidth\"] / x[\"SepalLength\"])).head()Out[88]: SepalLength SepalWidth PetalLength PetalWidth Name sepal_ratio0 5.1 3.5 1.4 0.2 Iris-setosa 0.6862751 4.9 3.0 1.4 0.2 Iris-setosa 0.6122452 4.7 3.2 1.3 0.2 Iris-setosa 0.6808513 4.6 3.1 1.5 0.2 Iris-setosa 0.6739134 5.0 3.6 1.4 0.2 Iris-setosa 0.720000In [90]: dfa = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})In [91]: dfa.assign(C=lambda x: x[\"A\"] + x[\"B\"], D=lambda x: x[\"A\"] + x[\"C\"])Out[91]: A B C D0 1 4 5 61 2 5 7 92 3 6 9 12 DataFrame的数学操作 DataFrame 可以直接和 Series 进行运算. 12345678910111213In [97]: df - df.iloc[0]Out[97]: A B C D0 0.000000 0.000000 0.000000 0.0000001 -1.359261 -0.248717 -0.453372 -1.7546592 0.253128 0.829678 0.010026 -1.9912343 -1.311128 0.054325 -1.724913 -1.6205444 0.573025 1.500742 -0.676070 1.3673315 -1.741248 0.781993 -1.241620 -2.0531366 -1.240774 -0.869551 -0.153282 0.0004307 -0.743894 0.411013 -0.929563 -0.2823868 -1.194921 1.320690 0.238224 -1.4826449 2.293786 1.856228 0.773289 -1.446531 也可以直接进行element-wise级别的运算 1234567891011121314151617181920212223242526272829303132333435363738394041In [98]: df * 5 + 2Out[98]: A B C D0 3.359299 -0.124862 4.835102 3.3811601 -3.437003 -1.368449 2.568242 -5.3921332 4.624938 4.023526 4.885230 -6.5750103 -3.196342 0.146766 -3.789461 -4.7215594 6.224426 7.378849 1.454750 10.2178155 -5.346940 3.785103 -1.373001 -6.8845196 -2.844569 -4.472618 4.068691 3.3833097 -0.360173 1.930201 0.187285 1.9692328 -2.615303 6.478587 6.026220 -4.0320599 14.828230 9.156280 8.701544 -3.851494In [99]: 1 / dfOut[99]: A B C D0 3.678365 -2.353094 1.763605 3.6201451 -0.919624 -1.484363 8.799067 -0.6763952 1.904807 2.470934 1.732964 -0.5830903 -0.962215 -2.697986 -0.863638 -0.7438754 1.183593 0.929567 -9.170108 0.6084345 -0.680555 2.800959 -1.482360 -0.5627776 -1.032084 -0.772485 2.416988 3.6145237 -2.118489 -71.634509 -2.758294 -162.5072958 -1.083352 1.116424 1.241860 -0.8289049 0.389765 0.698687 0.746097 -0.854483In [100]: df ** 4Out[100]: A B C D0 0.005462 3.261689e-02 0.103370 5.822320e-031 1.398165 2.059869e-01 0.000167 4.777482e+002 0.075962 2.682596e-02 0.110877 8.650845e+003 1.166571 1.887302e-02 1.797515 3.265879e+004 0.509555 1.339298e+00 0.000141 7.297019e+005 4.661717 1.624699e-02 0.207103 9.969092e+006 0.881334 2.808277e+00 0.029302 5.858632e-037 0.049647 3.797614e-08 0.017276 1.433866e-098 0.725974 6.437005e-01 0.420446 2.118275e+009 43.329821 4.196326e+00 3.227153 1.875802e+00 DataFrame 适配了所有的 Numpy function. 123456789101112131415161718192021222324252627282930313233343536In [108]: np.exp(df)Out[108]: A B C D0 1.312403 0.653788 1.763006 1.3181541 0.337092 0.509824 1.120358 0.2279962 1.690438 1.498861 1.780770 0.1799633 0.353713 0.690288 0.314148 0.2607194 2.327710 2.932249 0.896686 5.1735715 0.230066 1.429065 0.509360 0.1691616 0.379495 0.274028 1.512461 1.3187207 0.623732 0.986137 0.695904 0.9938658 0.397301 2.449092 2.237242 0.2992699 13.009059 4.183951 3.820223 0.310274In [109]: np.asarray(df)Out[109]: array([[ 0.2719, -0.425 , 0.567 , 0.2762], [-1.0874, -0.6737, 0.1136, -1.4784], [ 0.525 , 0.4047, 0.577 , -1.715 ], [-1.0393, -0.3706, -1.1579, -1.3443], [ 0.8449, 1.0758, -0.109 , 1.6436], [-1.4694, 0.357 , -0.6746, -1.7769], [-0.9689, -1.2945, 0.4137, 0.2767], [-0.472 , -0.014 , -0.3625, -0.0062], [-0.9231, 0.8957, 0.8052, -1.2064], [ 2.5656, 1.4313, 1.3403, -1.1703]])In [110]: ser = pd.Series([1, 2, 3, 4])In [111]: np.exp(ser)Out[111]: 0 2.7182821 7.3890562 20.0855373 54.598150dtype: float64 在终端上展示 默认情况下终端不会完全展示 DataFrame. 如果想要完全展示出来 DataFrame 的数据,一般有两种方法: 一个是转换为字符串 一个是转换为 Markdown 格式 一个是在 display setting 里调节参数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 转化为stringIn [126]: print(baseball.iloc[-20:, :12].to_string()) id player year stint team lg g ab r h X2b X3b80 89474 finlest01 2007 1 COL NL 43 94 9 17 3 081 89480 embreal01 2007 1 OAK AL 4 0 0 0 0 082 89481 edmonji01 2007 1 SLN NL 117 365 39 92 15 283 89482 easleda01 2007 1 NYN NL 76 193 24 54 6 084 89489 delgaca01 2007 1 NYN NL 139 538 71 139 30 085 89493 cormirh01 2007 1 CIN NL 6 0 0 0 0 086 89494 coninje01 2007 2 NYN NL 21 41 2 8 2 087 89495 coninje01 2007 1 CIN NL 80 215 23 57 11 188 89497 clemero02 2007 1 NYA AL 2 2 0 1 0 089 89498 claytro01 2007 2 BOS AL 8 6 1 0 0 090 89499 claytro01 2007 1 TOR AL 69 189 23 48 14 091 89501 cirilje01 2007 2 ARI NL 28 40 6 8 4 092 89502 cirilje01 2007 1 MIN AL 50 153 18 40 9 293 89521 bondsba01 2007 1 SFN NL 126 340 75 94 14 094 89523 biggicr01 2007 1 HOU NL 141 517 68 130 31 395 89525 benitar01 2007 2 FLO NL 34 0 0 0 0 096 89526 benitar01 2007 1 SFN NL 19 0 0 0 0 097 89530 ausmubr01 2007 1 HOU NL 117 349 38 82 16 398 89533 aloumo01 2007 1 NYN NL 87 328 51 112 19 199 89534 alomasa02 2007 1 NYN NL 8 22 1 3 1 0# In [127]: pd.set_option(\"display.max_colwidth\", 100) # optionalIn [128]: pd.set_option('display.max_rows', None)In [129]: pd.set_option('display.max_columns', None)In [130]: print(baseball.iloc[-20:, :12]) id player year stint team lg g ab r h X2b X3b80 89474 finlest01 2007 1 COL NL 43 94 9 17 3 081 89480 embreal01 2007 1 OAK AL 4 0 0 0 0 082 89481 edmonji01 2007 1 SLN NL 117 365 39 92 15 283 89482 easleda01 2007 1 NYN NL 76 193 24 54 6 084 89489 delgaca01 2007 1 NYN NL 139 538 71 139 30 085 89493 cormirh01 2007 1 CIN NL 6 0 0 0 0 086 89494 coninje01 2007 2 NYN NL 21 41 2 8 2 087 89495 coninje01 2007 1 CIN NL 80 215 23 57 11 188 89497 clemero02 2007 1 NYA AL 2 2 0 1 0 089 89498 claytro01 2007 2 BOS AL 8 6 1 0 0 090 89499 claytro01 2007 1 TOR AL 69 189 23 48 14 091 89501 cirilje01 2007 2 ARI NL 28 40 6 8 4 092 89502 cirilje01 2007 1 MIN AL 50 153 18 40 9 293 89521 bondsba01 2007 1 SFN NL 126 340 75 94 14 094 89523 biggicr01 2007 1 HOU NL 141 517 68 130 31 395 89525 benitar01 2007 2 FLO NL 34 0 0 0 0 096 89526 benitar01 2007 1 SFN NL 19 0 0 0 0 097 89530 ausmubr01 2007 1 HOU NL 117 349 38 82 16 398 89533 aloumo01 2007 1 NYN NL 87 328 51 112 19 199 89534 alomasa02 2007 1 NYN NL 8 22 1 3 1 0 NotePandas 常用的方法 实例 物体检测模型的结果: 有class: a,b,c,d,e 我们想看他的 confusion matrix. 123456789101112131415161718192021222324&gt;&gt;&gt; classes = ['a', 'b', 'c', 'd', 'e']# 初始值为 0, 这样的话就比较好直接进行 matrix 相加.# 如果Matrix 中有 NaN,直接相加会得到 NaN.&gt;&gt;&gt; cfm = pd.DataFrame(0, index=classes, columns=classes)&gt;&gt;&gt; cfm a b c d ea 0 0 0 0 0b 0 0 0 0 0c 0 0 0 0 0d 0 0 0 0 0e 0 0 0 0 0# 这里使用随机值代替模型评估统计的结果.&gt;&gt;&gt; cfm = cfm.applymap(lambda cell: np.random.randint(100)&gt;&gt;&gt; cfm a b c d ea 50 3 58 74 25b 3 63 19 98 90c 41 77 64 58 49d 45 45 81 64 40e 37 13 85 16 36# 我们可以直接把数字换成百分比. 比如坐标的 index 代表的是 groundtruth.# column 代表的是 prediction. 这个 confusion matrix 就可以用来检测分类性能.&gt;&gt;&gt; percentage_cfm = cfm.div(cfm.sum(axis=1))&gt;&gt;&gt;"},{"title":"Python中的数据结构","path":"/wiki/learn_python/advanced_foundation/index.html","content":"在创造Python以前，Guido曾为ABC语言贡献过代码。ABC语言是一个致力于为初学者设计编程环境的长达10年的研究项目，其中很多点子在现在看来都很有Python风格：序列的泛型操作、内置的元组和映射类型、用缩进来架构的源码、无需变量声明的强类型，等等。Python对开发者如此友好，根源就在这里。 序列构成的数组 不管是哪种数据结构，字符串、列表、字节序列、数组、XML元素，抑或是数据库查询结果，它们都共用一套丰富的操作：迭代、切片、排序，还有拼接。 内置序列类型概览 Python 标准库用 C 实现了丰富的序列类型: 容器序列扁平序列list, tuple, collections.deque这些序列能存放不同类型的数据.str, bytes, bytearray, memoryview, array.array 这类序列只能容纳一种类型, 这种序列其实是一段连续的内存空间, 但他里边只能存放基础类型. 容器类型还可以根据是否能被修改来分类: 可变序列不可变序列list, bytearray, array.array, collections.deque, memoeryview.tuple, str和bytes 列表推导和生成器表达式 通常的原则是，只用列表推导来创建新的列表，并且尽量保持简短。如果列表推导的代码超过了两行，你可能就要考虑是不是得用for循环重写了。列表推导不会再有变量泄漏的问题. 普通方法列表推导12345symbols = &quot;)(*(^(&amp;@&quot;code = []for symbol in symbols:\tcode.append(ord(symbol))print(code)123symbols = &quot;)(*(^(&amp;@&quot;code = [ord(symbol) for symbol in symbols]print(code) 在Python3中,表达式内部的变量和赋值只在局部起作用，表达式的上下文里的同名变量还可以被正常引用，局部变量并不会影响到它们, 比如下边的代码展示的. 1234567&gt;&gt;&gt; x = &#x27;ABC&#x27;&gt;&gt;&gt; dummy = [ord(x) for x in x]&gt;&gt;&gt; x&#x27;ABC&#x27;&gt;&gt;&gt; dummy[65, 66, 67]&gt;&gt; 使用列表推导生成FrenchCard 12# generate cards from A to Kranks = [str(n) for n in range(2, 11)] + list(&#x27;JQKA&#x27;) Source code you can find on GitHub 数据结构详解 字典 这里主要提一下defaultdict(). 在说明collections.defaultdict()之前，我们首先要提一下setdefault()方法。 Python 字典 setdefault() 函数和get()方法类似, 如果键不存在于字典中，将会添加键并将值设为默认值。 1234567dict.setdefault(key, default=None)# key – 查找的键值# default – 键不存在时，需要设置的默认值# return：如果字典中包含有给定键，则返回该键对应的值，否则返回为该键设置的值。# to use python defaultdictcollections.defaultdict([default_factory[, …]]) 因此, Python中通过Key访问字典，当Key不存在时，会引发‘KeyError’异常。为了避免这种情况的发生，可以使用collections类中的defaultdict()方法来为字典提供默认值。 上文中的 default_factory 的类型可以为int, set, list甚至 lambda function. 使用不同的类型创建 default_dict 的例子: 使用 list 创建 defaultdict 使用list作第一个参数，可以很容易将键-值对序列转换为列表字典。collections.defaultdict(list)使用起来效果和我们上面提到的dict.setdefault()比较相似，这种方法会和dict.setdefault()等价，但是要更快. 1234567s=[(&#x27;yellow&#x27;,1),(&#x27;blue&#x27;, 2), (&#x27;yellow&#x27;, 3), (&#x27;blue&#x27;, 4), (&#x27;red&#x27;, 1)]d=defaultdict(list)for k, v in s:\td[k].append(v)a=sorted(d.items())print(a)&gt;&gt;&gt; [(&#x27;blue&#x27;, [2, 4]), (&#x27;red&#x27;,* *[**1**]**), (&#x27;yellow&#x27;, [1, 3])] 使用 int 创建 defaultdict 将default_factory设为int时，可以被用来计数defaultdict(int) 这里的d其实是生成了一个默认为0的带key的数据字典。你可以想象成 d[key] = int default ，d[k]所以可以直接读取 d[“m”] += 1 就是d[“m”] 就是默认值 0+1 = 1 12345678910from collections import defaultdicts = &#x27;mississippi&#x27;d = defaultdict(int)for k in s: d[k] += 1print(&#x27; &#x27;,d)a=sorted(d.items())print(&#x27; &#x27;,a)&gt;&gt;&gt; defaultdict(&lt;class &#x27;int&#x27;&gt;, &#123;&#x27;m&#x27;: 1, &#x27;i&#x27;: 4, &#x27;s&#x27;: 4, &#x27;p&#x27;: 2&#125;)&gt;&gt;&gt; [(&#x27;i&#x27;, 4), (&#x27;m&#x27;, 1), (&#x27;p&#x27;, 2), (&#x27;s&#x27;, 4)] 使用 set 创建 defaultdict default_factory 设为set时，可以用defaultdict建立集合字典（a dictionary of sets） 12345678910from collections import defaultdicts = [(&#x27;red&#x27;, 1), (&#x27;blue&#x27;, 2), (&#x27;red&#x27;, 3), (&#x27;blue&#x27;, 4), (&#x27;red&#x27;, 1), (&#x27;blue&#x27;, 4)]d = defaultdict(set)for k, v in s: d[k].add(v)print(&#x27; &#x27;,d)a=sorted(d.items())print(&#x27; &#x27;,a)&gt;&gt;&gt; defaultdict(&lt;class &#x27;set&#x27;&gt;, &#123;&#x27;red&#x27;: &#123;1, 3&#125;, &#x27;blue&#x27;: &#123;2, 4&#125;&#125;)&gt;&gt;&gt; [(&#x27;blue&#x27;, &#123;2, 4&#125;), (&#x27;red&#x27;, &#123;1, 3&#125;)] 使用 lambda function 创建 defaultdict 使用 lambda function 的时候相当于把所有的 default value 都预先设置了一下. 123d = defaultdict(lambda: &quot;Not Present&quot;)print(d[&quot;c&quot;])&gt;&gt;&gt; Not Present"},{"title":"Python中常用的文件读取的方法比较","path":"/wiki/learn_python/advanced_foundation/readline_vs_readlines.html","content":"read() method: read entire contents and return it as a string. readline() method: read line by line, could use it when the file is large. a empty string will be returned when it reaches the end of the file. readlines() method: read entire contents, and return list of strings. Each element is a line. Method Pros Cons Read() 简单直接,小文件读取很方便. 1. 如果文件大的话会消耗大量内存 2. 文件的全部内容都在内存里,可能会造成性能问题. Readline() 可以处理大文件,甚至一些超出内存大小的文件. 读取小文件的时候效率不如其他方法高. Readlines() 小文件读取很方便, 返回值处理很直观. 1. 如果文件大的话会消耗大量内存 2. 文件的全部内容都在内存里,可能会造成性能问题. Readline() 不能识别文件中的 EOF,如果已经读到文件末尾,readline()依然会返回空字符串.因此需要手动去判断 readline 的输出来判断是否已经读完了文件."},{"title":"Python中的 string 和 byte string的区别","path":"/wiki/learn_python/advanced_foundation/str-vs-byte-str.html","content":"要了解python中基础类型str和byte str的区别,首先要了解计算机编码(Encodeing)和解码(Decoding). 计算机储存数据的单位是byte, 1 byte = 8 bit. 1 bit就是一个二进制码: 0 或 1. 任何可以储存到计算机中的数据都需要encode: 音乐需要被格式MP3, WAV等方式encode;图片使用PNG,JPEG等格式encode, 文字以ASCII, UTF-8等格式encode. Encode可以被理解为在计算机硬件上表示声音,图片,文字等信息的格式. 在Python中, byte string是一串字节序列. 他对人来说是不可读的. 另一方面一个char string(或者str),是一串字符序列.它对人来说是不可读的.一个字符不能直接被计算机储存, 它必须现先被 encode. 123&gt;&gt;&gt;# encode this str by using ASCII&gt;&gt;&gt;&#x27;this is a string&#x27;.encode(&#x27;ASCII&#x27;)b&#x27;this is a string&#x27; 上边的代码虽然把 byte string 打印了出来,但是这是 python 的一个功能. 本质上 byte string 依旧是人类不能阅读的.因此 python 在 str 前边加了代表 byte string 的b. byte string 也可以被 decode 回来: 123&gt;&gt;&gt;# decode this str by using ASCII&gt;&gt;&gt; b&#x27;I am a string&#x27;.decode(&#x27;ASCII&#x27;)&#x27;I am a string&#x27; 然后python 就直接返回一个正常的 char string. Encoding 和 Decoding 是相反的操作. 为什么要用 byte string 什么是编码 因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。最早的计算机在设计时采用8个比特（bit）作为一个字节（byte），所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255），如果要表示更大的整数，就必须用更多的字节。比如两个字节可以表示的最大整数是65535，4个字节可以表示的最大整数是4294967295。 由于计算机是美国人发明的，因此，最早只有127个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。 如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。 使用 byte string 一般 Python 会默认使用 Unicode 来编码解码. 使用bytes类型，实质上是告诉Python，不需要它帮你自动地完成编码和解码的工作，而是用户自己手动进行，并指定编码格式。而且由于Python的字符串类型是str，在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者保存到磁盘上，就需要把str变为以字节为单位的bytes。"},{"title":"Python 中的枚举","path":"/wiki/learn_python/advanced_tricky/index.html","content":"什么是枚举? 枚举(enumerate)是Python内置函数。在python中枚举是一种类（Enum,IntEnum），存放在enum模块中。枚举类型可以给一组标签赋予一组特定的值。 枚举的特点 枚举类中不能存在相同的标签名 枚举是可迭代的 不同的枚举标签可以对应相同的值，但它们都会被视为该值对应第一个标签的别名 如果要限制定义枚举时，不能定义相同值的成员。可以使用装饰器@unique 枚举成员之间不能进行大小比较，可进行等值和同一性比较 枚举成员为单例，不可实例化，不可更改 定义枚举 01. 直接创建类似枚举结构的变量 12345&gt;&gt;&gt; RED, GREEN, YELLOW = range(3)&gt;&gt;&gt; RED0&gt;&gt;&gt; GREEN1 02.使用 Enum 来创建枚举类 123456789101112131415&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; class Day(Enum):... MONDAY = 1... TUESDAY = 2... WEDNESDAY = 3... THURSDAY = 4... FRIDAY = 5... SATURDAY = 6... SUNDAY = 7...&gt;&gt;&gt; type(Day.MONDAY)&lt;enum &#x27;Day&#x27;&gt;&gt;&gt; type(Day.TUESDAY)&lt;enum &#x27;Day&#x27;&gt; 也可以结合 01, 和 02 一起创建枚举的类. 12345678910111213&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; class Season(Enum):... WINTER, SPRING, SUMMER, FALL = range(1, 5)...&gt;&gt;&gt; list(Season)[ &lt;Season.WINTER: 1&gt;, &lt;Season.SPRING: 2&gt;, &lt;Season.SUMMER: 3&gt;, &lt;Season.FALL: 4&gt;] 继承 Enmu 的类还可以被再次继承: 123456789101112131415161718&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; import string&gt;&gt;&gt; class BaseTextEnum(Enum):... def as_list(self):... try:... return list(self.value)... except TypeError:... return [str(self.value)]...&gt;&gt;&gt; class Alphabet(BaseTextEnum):... LOWERCASE = string.ascii_lowercase... UPPERCASE = string.ascii_uppercase...&gt;&gt;&gt; Alphabet.LOWERCASE.as_list()[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, ..., &#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;] 03.使用 Enum 的 API 来创建枚举类 可以使用 list 来快速创建映射. 123456789101112131415161718192021&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; HTTPStatusCode = Enum(... value=&quot;HTTPStatusCode&quot;,... names=[... (&quot;OK&quot;, 200),... (&quot;CREATED&quot;, 201),... (&quot;BAD_REQUEST&quot;, 400),... (&quot;NOT_FOUND&quot;, 404),... (&quot;SERVER_ERROR&quot;, 500),... ],... )&gt;&gt;&gt; list(HTTPStatusCode)[ &lt;HTTPStatusCode.OK: 200&gt;, &lt;HTTPStatusCode.CREATED: 201&gt;, &lt;HTTPStatusCode.BAD_REQUEST: 400&gt;, &lt;HTTPStatusCode.NOT_FOUND: 404&gt;, &lt;HTTPStatusCode.SERVER_ERROR: 500&gt;] 使用auto() 的方法来帮助创建枚举 Python 的enum模块提供了一个方便的函数auto()，该函数允许您为枚举成员设置自动值。此函数的默认行为是将连续整数值分配给成员. 您需要为auto()所需的每个自动值调用一次。您还可以auto()与具体值组合，就像您在本例中使用Day.WEDNESDAYand所做的那样Day.SUNDAY。 默认情况下，auto()从 开始为每个目标成员分配连续整数1。您可以通过覆盖该.generate_next_value()方法来调整此默认行为，该方法auto()在后台使用来生成自动值。 auto() 方法可以自动检测数字. 12345678910111213141516171819202122&gt;&gt;&gt; from enum import auto, Enum&gt;&gt;&gt; class Day(Enum):... MONDAY = auto()... TUESDAY = auto()... WEDNESDAY = 3... THURSDAY = auto()... FRIDAY = auto()... SATURDAY = auto()... SUNDAY = 7...&gt;&gt;&gt; list(Day)[ &lt;Day.MONDAY: 1&gt;, &lt;Day.TUESDAY: 2&gt;, &lt;Day.WEDNESDAY: 3&gt;, &lt;Day.THURSDAY: 4&gt;, &lt;Day.FRIDAY: 5&gt;, &lt;Day.SATURDAY: 6&gt;, &lt;Day.SUNDAY: 7&gt;] auto() 方法也可以可以自动检测字母. 123456789101112131415161718&gt;&gt;&gt; from enum import Enum, auto&gt;&gt;&gt; class CardinalDirection(Enum):... def _generate_next_value_(name, start, count, last_values):... return name[0]... NORTH = auto()... SOUTH = auto()... EAST = auto()... WEST = auto()...&gt;&gt;&gt; list(CardinalDirection)[ &lt;CardinalDirection.NORTH: &#x27;N&#x27;&gt;, &lt;CardinalDirection.SOUTH: &#x27;S&#x27;&gt;, &lt;CardinalDirection.EAST: &#x27;E&#x27;&gt;, &lt;CardinalDirection.WEST: &#x27;W&#x27;&gt;] 使用别名和唯一值创建枚举类 你可以创建两个或多个成员具有相同常量值的枚举。冗余成员称为别名，在某些情况下可能很有用。例如，假设您有一个包含一组操作系统 (OS) 的枚举，如以下代码所示: 12345678910111213141516171819202122232425&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; class OperatingSystem(Enum):... UBUNTU = &quot;linux&quot;... MACOS = &quot;darwin&quot;... WINDOWS = &quot;win&quot;... DEBIAN = &quot;linux&quot;...&gt;&gt;&gt; # 别名不展示&gt;&gt;&gt; list(OperatingSystem)[ &lt;OperatingSystem.UBUNTU: &#x27;linux&#x27;&gt;, &lt;OperatingSystem.MACOS: &#x27;darwin&#x27;&gt;, &lt;OperatingSystem.WINDOWS: &#x27;win&#x27;&gt;]&gt;&gt;&gt; # 访问别名需要使用到参数: __members__&gt;&gt;&gt; list(OperatingSystem.__members__.items())[ (&#x27;UBUNTU&#x27;, &lt;OperatingSystem.UBUNTU: &#x27;linux&#x27;&gt;), (&#x27;MACOS&#x27;, &lt;OperatingSystem.MACOS: &#x27;darwin&#x27;&gt;), (&#x27;WINDOWS&#x27;, &lt;OperatingSystem.WINDOWS: &#x27;win&#x27;&gt;), (&#x27;DEBIAN&#x27;, &lt;OperatingSystem.UBUNTU: &#x27;linux&#x27;&gt;)] 前文提到可以使用@unique装饰器来限制定义枚举. 这里是一个例子: 123456789101112&gt;&gt;&gt; from enum import Enum, unique&gt;&gt;&gt; @unique... class OperatingSystem(Enum):... UBUNTU = &quot;linux&quot;... MACOS = &quot;darwin&quot;... WINDOWS = &quot;win&quot;... DEBIAN = &quot;linux&quot;...Traceback (most recent call last): ...ValueError: duplicate values in &lt;enum &#x27;OperatingSystem&#x27;&gt;: DEBIAN -&gt; UBUNTU 枚举的使用 访问枚举成员 常见的三种访问枚举成员的方法: 使用符号 . 引用方法: 使用 EnmuClass(&quot;key&quot;) 类似字典的访问方法: EnmuClass[&quot;key&quot;] 12345678910111213141516171819202122&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; class CardinalDirection(Enum):... NORTH = &quot;N&quot;... SOUTH = &quot;S&quot;... EAST = &quot;E&quot;... WEST = &quot;W&quot;...&gt;&gt;&gt; # Dot notation&gt;&gt;&gt; # Python 枚举的成员是其包含类的实例。在枚举类解析期间，每个成员都会自动提供一个.name属性，该属性将成员的名称保存为字符串。成员还获得一个.value属性，该属性将分配给成员本身的值存储在类定义中。&gt;&gt;&gt; # 因此可以使用 CardinalDirection.NORTH.name 或者 CardinalDirection.NORTH.value得到一个对应的字符串.&gt;&gt;&gt; CardinalDirection.NORTH&lt;CardinalDirection.NORTH: &#x27;N&#x27;&gt;&gt;&gt;&gt; # Call notation&gt;&gt;&gt; CardinalDirection(&quot;N&quot;)&lt;CardinalDirection.NORTH: &#x27;N&#x27;&gt;&gt;&gt;&gt; # Subscript notation&gt;&gt;&gt; CardinalDirection[&quot;NORTH&quot;]&lt;CardinalDirection.NORTH: &#x27;N&#x27;&gt; 遍历枚举的成员 123456789101112131415161718192021222324252627&gt;&gt;&gt; for name in Flavor.__members__:... print(name)...VANILLACHOCOLATEMINT&gt;&gt;&gt; for name in Flavor.__members__.keys():... print(name)...VANILLACHOCOLATEMINT&gt;&gt;&gt; for member in Flavor.__members__.values():... print(member)...Flavor.VANILLAFlavor.CHOCOLATEFlavor.MINT&gt;&gt;&gt; for name, member in Flavor.__members__.items():... print(name, &quot;-&gt;&quot;, member)...VANILLA -&gt; Flavor.VANILLACHOCOLATE -&gt; Flavor.CHOCOLATEMINT -&gt; Flavor.MINT 枚举中的比较 可以使用if…elif语句和match…case语句中使用枚举表明来比较枚举成员。默认情况下，枚举支持两种类型的比较运算符： 身份运算符，使用isandis not运算符 算数运算符，使用==and!=运算符 身份比较依赖于每个枚举成员都是其枚举类的单例实例这一事实. is此特性允许使用和运算符对成员进行快速且廉价的身份比较is not。 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; class AtlanticAveSemaphore(Enum):... RED = 1... YELLOW = 2... GREEN = 3... PEDESTRIAN_RED = 1... PEDESTRIAN_GREEN = 3&gt;&gt;&gt; red = AtlanticAveSemaphore.RED&gt;&gt;&gt; red == AtlanticAveSemaphore.REDTrue&gt;&gt;&gt; red != AtlanticAveSemaphore.REDFalse&gt;&gt;&gt; yellow = AtlanticAveSemaphore.YELLOW&gt;&gt;&gt; yellow == redFalse&gt;&gt;&gt; yellow is redFalse&gt;&gt;&gt; yellow != redTrue&gt;&gt;&gt; yellow is not redTrue&gt;&gt;&gt; pedestrian_red = AtlanticAveSemaphore.PEDESTRIAN_RED&gt;&gt;&gt; red == pedestrian_redTrue&gt;&gt;&gt; red is pedestrian_redTrue 12345678910111213141516171819202122232425&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; class Semaphore(Enum):... RED = 1... YELLOW = 2... GREEN = 3...&gt;&gt;&gt; def handle_semaphore(light):... if light is Semaphore.RED:... print(&quot;You must stop!&quot;)... elif light is Semaphore.YELLOW:... print(&quot;Light will change to red, be careful!&quot;)... elif light is Semaphore.GREEN:... print(&quot;You can continue!&quot;)...&gt;&gt;&gt; handle_semaphore(Semaphore.GREEN)You can continue!&gt;&gt;&gt; handle_semaphore(Semaphore.YELLOW)Light will change to red, be careful!&gt;&gt;&gt; handle_semaphore(Semaphore.RED)You must stop! 枚举排序 默认情况下，Python 的枚举不支持比较运算符，如&gt;、&lt;、&gt;=和&lt;=. 这就是为什么您不能直接使用内置sorted()函数对枚举成员进行排序的原因，如下例所示： 12345678910111213&gt;&gt;&gt; from enum import Enum&gt;&gt;&gt; class Season(Enum):... SPRING = 1... SUMMER = 2... AUTUMN = 3... WINTER = 4...&gt;&gt;&gt; sorted(Season)Traceback (most recent call last): ...TypeError: &#x27;&lt;&#x27; not supported between instances of &#x27;Season&#x27; and &#x27;Season&#x27; 来排序的时候,需要手动映射枚举成员为排序的 key 来进行排序. 123456789101112131415&gt;&gt;&gt; sorted(Season, key=lambda season: season.value)[ &lt;Season.SPRING: 1&gt;, &lt;Season.SUMMER: 2&gt;, &lt;Season.AUTUMN: 3&gt;, &lt;Season.WINTER: 4&gt;]&gt;&gt;&gt; sorted(Season, key=lambda season: season.name)[ &lt;Season.AUTUMN: 3&gt;, &lt;Season.SPRING: 1&gt;, &lt;Season.SUMMER: 2&gt;, &lt;Season.WINTER: 4&gt;] 其他枚举类 构建整数枚举：IntEnum 123456789101112131415161718192021&gt;&gt;&gt; from enum import IntEnum&gt;&gt;&gt; class Size(IntEnum):... S = 1... M = 2... L = 3... XL = &quot;4&quot; #可以成功被转换...&gt;&gt;&gt; list(Size)[&lt;Size.S: 1&gt;, &lt;Size.M: 2&gt;, &lt;Size.L: 3&gt;, &lt;Size.XL: 4&gt;]&gt;&gt;&gt; class Size(IntEnum):... S = 1... M = 2... L = 3... XL = &quot;4.o&quot; #不可以被转换...Traceback (most recent call last): ...ValueError: invalid literal for int() with base 10: &#x27;4.o&#x27; 使用 Flag 来构建枚举 使用 Flag 的好处之一就是可以让一个枚举类型包含另外几种类型.比如以下一个例子: 该枚举在给定的应用程序中包含一组用户角色。此枚举的成员包含整数值，您可以使用按位 OR 运算符 ( |) 组合这些值。例如，名为 John 的用户同时具有USER和SUPERVISOR角色。请注意，存储在其中的对象john_roles是您的Role枚举的成员。 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; from enum import IntFlag&gt;&gt;&gt; class Role(IntFlag):... OWNER = 8... POWER_USER = 4... USER = 2... SUPERVISOR = 1... ADMIN = OWNER | POWER_USER | USER | SUPERVISOR...&gt;&gt;&gt; john_roles = Role.USER | Role.SUPERVISOR&gt;&gt;&gt; john_roles&lt;Role.USER|SUPERVISOR: 3&gt;&gt;&gt;&gt; type(john_roles)&lt;enum &#x27;Role&#x27;&gt;&gt;&gt;&gt; if Role.USER in john_roles:... print(&quot;John, you&#x27;re a user&quot;)...John, you&#x27;re a user&gt;&gt;&gt; if Role.SUPERVISOR in john_roles:... print(&quot;John, you&#x27;re a supervisor&quot;)...John, you&#x27;re a supervisor&gt;&gt;&gt; Role.OWNER in Role.ADMINTrue&gt;&gt;&gt; Role.SUPERVISOR in Role.ADMINTrue","tags":[null]},{"title":"二叉树intro","path":"/wiki/leetcode/binary_tree/index.html","content":"二叉树特点是每个节点最多只能有两棵子树，且有左右之分。二叉树是n个有限元素的集合，该集合或者为空、或者由一个称为根（root）的元素及两个不相交的、被分别称为左子树和右子树的二叉树组成，是有序树。 二叉树的遍历方式无非就是(1)前序遍历, (2)中序遍历和(3)后序遍历. 总的来说打印一个二叉树有两种方式.一种是(1)横向打印,一种是(2)纵向打印. 遍历方式 二叉树的遍历方式无非就是(1)前序遍历, (2)中序遍历,(3)后序遍历和(4)层序遍历. 前序,中序,后序遍历 二叉树的遍历方式很简单,只是递归途中的二级入口位置不同而已. def travel(node): if node is None: return print(node.val) # 前序遍历位置 travel(node.left) print(node.val) # 中序遍历位置 travel(node.right) print(node.val) # 后序遍历位置 层遍历 层序遍历 二叉树的结构导致他并不能存储他的一些信息,比如深度. 当一层一层遍历一个二叉树的时候, 递归方法也很难找到清晰的运行层级关系.换句话说,我们虽然可以按层来打印每个 node 但是不知道每一层的结尾在哪里. 因此当有层级关系的处理的时候,我们需要手动来储存层的数据. 1234567891011121314def travel(root):“”“这个方法没有传递层级信息因此在处理 node 的时候根本不知道是哪一层.”“”queue = [root]while queue:node = queue.pop(-1)print(node.val) # 层级遍历if node.left:queue.append(node.left)if node.right:queue.append(node.right) 按层打印二叉树 要想按层来打印二叉树,需要知道二叉树在哪一层. 我们可以把整个树都遍历一遍把每个 node 的层级信息都存起来. 123456789101112131415161718def bfs(node, level, queue):if not node:returnqueue.append((level, node.val))if node.left:bfs(node.left, level + 1, queue)if node.right:bfs(node.right, level + 1, queue)depth = 0queue = []bfs(root, level=0, queue=queue)while queue:pop_node = queue.pop(0)if pop_node[0] &gt; depth:print()depth += 1print(pop_node[1], end=&#x27; &#x27;)","tags":[null]},{"title":"LeetCode 面试题 04.12. 二叉树的最短求和路径","path":"/wiki/leetcode/binary_tree/interview_04_12_paths_with_sum_LCCI.html","content":"看完题目的第一反应就是用剪枝的思想来穷举. 如果只考虑到所有 Node 都是正数的情况的话,其实可以快很多. 不过考虑到有一些 node 是负数,因此每次都要遍历完整个树. 一般的剪枝思想其实不会动 root. 但是这个题目是要列出来所有的可能, 这样的话就是用每个 node 都为根来进行穷举. 代码如下: 123456789101112131415161718192021222324252627class Solution: def pathSum(self, root: TreeNode, sum_: int) -&gt; int: if not root: return 0 ans = [] current_vals = [] def travel(node, target_num, current_vals, ans): if node is None: return current_vals.append(node.val) if sum(current_vals) == target_num: ans.append(current_vals.copy()) travel(node.left, target_num, current_vals, ans) if node.left: current_vals.pop() travel(node.right, target_num, current_vals, ans) if node.right: current_vals.pop() queue = [root] while queue: node = queue.pop(-1) travel(node, sum_, [], ans) if node.left: queue.append(node.left) if node.right: queue.append(node.right) return len(ans) 考虑到树的很多枝其实是经过多次计算的, 因此可以用动态规划在继续优化","tags":[null]},{"title":"LeetCode 100. 相同的树","path":"/wiki/leetcode/binary_tree/leetcode_100_same_tree.html","content":"这个只要会遍历二叉树就行了,不管用什么遍历方式,直接在遍历的时候比较两个 node 就行了. 代码如下: 123456789101112131415class Solution: def isSameTree(self, p: Optional[TreeNode], q: Optional[TreeNode]) -&gt; bool: def travel(node1, node2): if node1 is None and node2 is None: return True if node1 is None and node2 is not None: return False if node1 is not None and node2 is None: return False if node1.val != node2.val: return False return travel(node1.left, node2.left) \\ and travel(node1.right, node2.right) return travel(p, q)","tags":[null]},{"title":"LeetCode 101. 对称二叉树","path":"/wiki/leetcode/binary_tree/leetcode_101_symmetric_tree.html","content":"可以把树看成两个树. 根节点不用管. 直接同时遍历根节点的左子树和右子树,然后看左子树的左边等不等于右子树的右边, 以此类推. 其实就是简单的后续遍历,在遍历的时候看看左子树是否等于右子树. 代码如下: 123456789101112131415class Solution: def isSymmetric(self, root: Optional[TreeNode]) -&gt; bool: if root is None: return True def travel(node1, node2): if node1 is None and node2 is None: return True if node1 is None and node2 is not None: return False if node1 is not None and node2 is None: return False if node1.val != node2.val: return False return travel(node1.left, node2.right) and travel(node1.right, node2.left) return travel(root.left, root.right)","tags":[null]},{"title":"LeetCode 104. 二叉树的最大深度","path":"/wiki/leetcode/binary_tree/leetcode_104_maximum_depth_of_binary_tree.html","content":"废话不多说,二叉树概念的基础. 直接用后序遍历,两行代码搞定. 代码如下: 12345678class Solution: def maxDepth(self, root: Optional[TreeNode]) -&gt; int: if root is None: return 0 return max(self.maxDepth(root.left), self.maxDepth(root.right)) + 1","tags":[null]},{"title":"LeetCode 114. 二叉树展开为链表","path":"/wiki/leetcode/binary_tree/leetcode_114_flatten_binary_tree_to_linked_list.html","content":"总得思路就是把左子树的尾巴返回出来接到右子树的头,然后再把左子树的头赋值给父节点的右子树. 难点在于怎么取出来左子树的尾巴. 这里和最短求和路径相似,都是从上往下遍历,但是从下往上处理.这里还少了判断求和路径的值,只是把该分路径下的尾巴节点返回. 代码如下: 12345678910111213141516171819202122232425262728class Solution: def flatten(self, root: Optional[TreeNode]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify root in-place instead. &quot;&quot;&quot; if root is None: return def find_tail(node): if node is None: return left_tail = find_tail(node.left) right_tail = find_tail(node.right) if left_tail: if right_tail: left_tail.right = node.right node.right = node.left node.left = None return right_tail else: node.right = node.left node.left = None return left_tail if right_tail: return right_tail return node # don&#x27;t need this return value, unless we need to link two trees. find_tail(root)","tags":[null]},{"title":"LeetCode 129. 求根节点到叶节点数字之和","path":"/wiki/leetcode/binary_tree/leetcode_129_sum_root_to_leaf_numbers.html","content":"该数据结构相当于每一层深度代表一个位. 比如深度为 1 的时候代表最高位最深的时候代表个位. 以此类推. 总得思路可以是直接 DFS 到叶子节点然后吧存的数字添加到一个地方. 然后进行计算, 也可以在到叶子节点的时候不储存直接计算. 代码如下: 12345678910111213141516171819class Solution: def sumNumbers(self, root: Optional[TreeNode]) -&gt; int: self.total = 0 if root is None: return ans def travel(node, current): # pre-condition: node is not None current_update = f&quot;&#123;current&#125;&#123;node.val&#125;&quot; # if is leaf node, add current to total if node.left is None and node.right is None: self.total += int(current_update) if node.left: travel(node.left, current_update) if node.right: travel(node.right, current_update) travel(root, &quot;&quot;) return self.total","tags":[null]},{"title":"LeetCode 117. 填充每个节点的下一个右侧节点指针 II","path":"/wiki/leetcode/binary_tree/leetcode_117_populating_next_right_pointers_in_each_node_2.html","content":"虽然这道题被标为中等难度, 但是也是一个 bfs 就可以解决的. 只是需要存一下深度信息然后作比较就好了. 代码如下: 12345678910111213141516171819class Solution: def connect(self, root: &#x27;Node&#x27;) -&gt; &#x27;Node&#x27;: if root is None: return def bfs(): level = 0 node_index = 0 level_index = 1 queue = [(root, level)] while queue: node, current_level = queue.pop(0) if queue and queue[0][level_index] == current_level: node.next = queue[0][node_index] if node.left: queue.append((node.left, current_level+1)) if node.right: queue.append((node.right, current_level+1)) bfs() return root","tags":[null]},{"title":"LeetCode 112. 路径总和","path":"/wiki/leetcode/binary_tree/leetcode_112_path_sum.html","content":"思路很简单 dfs 然后检查是否有一条路径返回找到了就ok. 代码如下: 123456789class Solution: def hasPathSum(self, root: Optional[TreeNode], targetSum: int) -&gt; bool: if root is None: return False if root.left is None and root.right is None and targetSum - root.val == 0: return True return self.hasPathSum(root.left, targetSum - root.val) or \\ self.hasPathSum(root.right, targetSum - root.val)","tags":[null]},{"title":"LeetCode 226. 翻转二叉树","path":"/wiki/leetcode/binary_tree/leetcode_226_invert_binary_tree.html","content":"思路也很简单,就是后续遍历的同时把所有的左右节点换一下位置就 ok. 代码如下: 123456789class Solution: def invertTree(self, root: Optional[TreeNode]) -&gt; Optional[TreeNode]: if root is None: return left = self.invertTree(root.left) right = self.invertTree(root.right) root.left = right root.right = left return root","tags":[null]},{"title":"LeetCode 230. 二叉搜索树中第K小的元素","path":"/wiki/leetcode/binary_tree/leetcode_230_Kth_smallest_element_in_BST.html","content":"一个 BST 的前序遍历就是一个有序数组. 因此这道题其实只需要返回前序遍历的第 K 个值. 代码如下: 1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def kthSmallest(self, root: Optional[TreeNode], k: int) -&gt; int: stack = [root] curr = root.left while True: if curr: stack.append(curr) curr = curr.left elif stack: curr = stack.pop() k -= 1 if k == 0: return curr.val curr = curr.right else: break return 0","tags":[null]},{"title":"Python 中的正则表达式","path":"/wiki/learn_python/regular_expression/index.html","content":"","tags":[null]},{"title":"LeetCode 236. 二叉树的最近公共祖先","path":"/wiki/leetcode/binary_tree/leetcode_236_lowest_common_ancestor_of_binary_tree.html","content":"找最近的公共祖先其实就是用后序遍历. 虽然是从上往下遍历,但是需要从下往上处理数据. 这里可以把找到的 node 返回,然后对于一个 node 看是否左子树和右子树都有返回值. 如果是的话,则该 node 就是两个 node 的最近的公共祖先.(因为是后续遍历所以是最近的.) 如果没有找到目标 node 则返回空即可. 代码如下: 1234567891011121314class Solution: def lowestCommonAncestor(self, root: &#x27;TreeNode&#x27;, p: &#x27;TreeNode&#x27;, q: &#x27;TreeNode&#x27;) -&gt; &#x27;TreeNode&#x27;: if root is None: return if root == p or root == q: return root left = self.lowestCommonAncestor(root.left, p, q) right = self.lowestCommonAncestor(root.right, p, q) if left and right: return root if left is None and right is None: return return left if left else right","tags":[null]}]